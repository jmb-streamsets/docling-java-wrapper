<doctag><page_header><loc_15><loc_144><loc_30><loc_356>arXiv:2512.01970v1  [cs.AI]  1 Dec 2025</page_header>
<page_header><loc_88><loc_18><loc_145><loc_24>Work in Progress</page_header>
<section_header_level_1><loc_89><loc_52><loc_411><loc_86>FROM ATOMIC TO COMPOSITE: REINFORCEMENT LEARNING ENABLES GENERALIZATION IN COMPLEMENTARY REASONING</section_header_level_1>
<text><loc_93><loc_99><loc_339><loc_113>Sitao Cheng 1 † , Xunjian Yin 2 , Ruiwen Zhou 3 , Yuxuan Li 1 , Xinyi Wang 4 , Liangming Pan 5 † , William Yang Wang 6 , Victor Zhong 1 †</text>
<text><loc_93><loc_116><loc_96><loc_120>1</text>
<text><loc_97><loc_118><loc_171><loc_123>University of Waterloo</text>
<footnote><loc_93><loc_124><loc_164><loc_130>4 Princeton University</footnote>
<text><loc_247><loc_116><loc_378><loc_130>3 National University of Singapore 6 University of California, Santa Barbara</text>
<text><loc_174><loc_124><loc_177><loc_127>5</text>
<text><loc_181><loc_116><loc_185><loc_120>2</text>
<text><loc_185><loc_118><loc_239><loc_123>Duke University</text>
<text><loc_178><loc_125><loc_237><loc_130>Peking University</text>
<text><loc_93><loc_135><loc_414><loc_140>{ sitao.cheng, victor.zhong } @uwaterloo.com liangmingpan@pku.edu.cn</text>
<section_header_level_1><loc_227><loc_160><loc_273><loc_166>ABSTRACT</section_header_level_1>
<text><loc_118><loc_177><loc_382><loc_369>Reinforcement Learning (RL) following Supervised Fine-Tuning (SFT) has become the standard paradigm for post-training Large Language Models (LLMs). However, the mechanism by which RL contributes to reasoning capabilitieswhether it incentivizes the synthesis of new skills or merely amplifies existing behaviors-remains a subject of intense debate. In this work, we investigate this question through the lens of Complementary Reasoning , a complex task that requires integrating internal parametric knowledge with external contextual information. Using a controlled synthetic dataset of human biographies, we strictly decouple this ability into two atomic skills: Parametric Reasoning (relying on internal knowledge encoded in model parameters) and Contextual Reasoning (depending on novel information provided in the context window). To rigorously assess capability boundaries, we evaluate generalization across three distinct levels of difficulty: I.I.D., Composition, and Zero-shot settings . We find that while SFT is sufficient for in-distribution performance, it struggles with out-of-distribution generalization, particularly in Zero-shot settings where relational combinations are novel. Crucially, we identify the SFT Generalization Paradox : Models supervised solely on the composite task achieve near-perfect in-distribution accuracy (90%) but collapse on out-of-distribution generalization (18%), indicating their reliance on rote memorization of path shortcuts. In contrast, we find that RL acts as a reasoning synthesizer rather than a probability amplifier. However, we uncover a strict atomic prerequisite: RL can only synthesize these complex strategies if the base model has first mastered the independent atomic skills (Parametric and Contextual) via SFT. These findings challenge the view of RL as a mere amplifier, suggesting that given sufficient atomic foundations, RL can actively synthesize complex reasoning strategies from learned primitives without explicit supervision on such complex strategies. This indicates that decoupled atomic training followed by RL offers a scalable path to generalization for complex reasoning tasks. Code and data will be at https://github.com/sitaocheng/from atomic to composite.</text>
<section_header_level_1><loc_88><loc_385><loc_168><loc_392>1 INTRODUCTION</section_header_level_1>
<text><loc_88><loc_402><loc_412><loc_449>The rapid evolution of Large Language Models (LLMs) has been fundamentally driven by advanced post-training strategies, specifically an initial Supervised Fine-Tuning (SFT) stage followed by a Reinforcement Learning (RL) stage (Achiam et al., 2023; Team et al., 2024; Guo et al., 2025). While SFT is effective at establishing behavioral norms and imparting foundational knowledge, it fundamentally relies on maximum likelihood estimation, which tends to favor the memorization of the training distribution. This often results in poor generalization outside the training domain (Wang et al., 2025; Chu et al., 2025). Conversely, RL, guided by reward signals, is hypothesized</text>
<footnote><loc_99><loc_456><loc_167><loc_462>† Corresponding author.</footnote>
<page_footer><loc_248><loc_476><loc_252><loc_481>1</page_footer>
<page_break>
<page_header><loc_88><loc_18><loc_145><loc_24>Work in Progress</page_header>
<picture><loc_89><loc_52><loc_411><loc_188><caption><loc_88><loc_193><loc_412><loc_236>Performance on Complementary Reasoning Test Set Figure 1: The SFT Generalization Paradox: Models trained on atomic skills generalize better via RL than models trained directly on the composite task. (a) Example of Complementary Reasoning requiring the skills of both Parametric and Contextual Reasoning. (b) Evaluation protocol through three levels of difficulty with sufficient training data of different reasoning types. and denotes seen and unseen relation (path), respectively. (c) We find that RL composes new skills under the condition that the base model has sufficient atomic abilities.</caption></picture>
<text><loc_88><loc_244><loc_412><loc_284>to transform the model's distribution towards goal-oriented problem-solving strategies (Schulman et al., 2017). While recent 'Thinking' models (Guo et al., 2025) demonstrate that extensive RL can improve reasoning via test-time computation, the mechanism of this improvement remains debated: Does RL incentivizes the acquisition of genuinely new logic circuits (Yuan et al., 2025; Liu et al., 2025), or merely amplify correct traces found in the pre-training and SFT distributions (Wu & Choi, 2025; Yue et al., 2025)? Our work isolates this mechanism by controlling the atomic priors.</text>
<text><loc_88><loc_289><loc_412><loc_399>To study the true nature of RL's contribution, we focus on a common practical task, knowledgeintensive reasoning (Guti´ errez et al., 2025; An et al., 2025; Zhuang et al., 2024). In real-world scenarios, systems often encounter 'missing link' failures: for example, in Figure 1(a), a RetrievalAugmented Generation (RAG) system might retrieve a document about a 'Chief Editor', but fail to link it to the chief editor's 'business partner' stored in the model's parametric memory. We define this capability as Complementary Reasoning (COMP) : the ability to seamlessly bridge internal parametric knowledge with external contextual information. For example, answering 'What is the occupation of the business partner of Global View's new Chief Editor? ' requires the model to seamlessly integrate the external information ( e.g., Amina Khan is the Chief...) with the internal knowledge ( e.g., Ben Carter is a business partner...). While human intelligence intuitively performs this composition-seamlessly leveraging known facts alongside new context-LLMs often struggle to generalize when required to utilize both parametric and contextual knowledge simultaneously (Cheng et al., 2024a; Yin et al., 2023). While LLMs are known for their fluency in using either parametric or contextual knowledge, Complementary Reasoning serves as an ideal testbed for evaluating how post-training strategies can bridge the generalization gap between the isolated atomic skills and complex, compositional reasoning.</text>
<text><loc_88><loc_404><loc_412><loc_416>In this paper, we investigate the minimal sufficient conditions required for LLMs to generalize on complementary reasoning. We specifically aim to answer two research questions:</text>
<unordered_list><list_item><loc_88><loc_421><loc_411><loc_427>RQ1: What training strategies are necessary for generalization to Complementary Reasoning?</list_item>
<list_item><loc_88><loc_432><loc_412><loc_444>RQ2: Can RL synthesize complex reasoning strategies from atomic primitives, or does it merely amplify existing memorization?</list_item>
</unordered_list>
<text><loc_88><loc_450><loc_412><loc_462>Investigating these questions using standard open-domain benchmarks ( e.g. HotpotQA (Yang et al., 2018) and PopQA (Mallen et al., 2023)) is fundamentally limited by data contamination. In web-</text>
<page_footer><loc_248><loc_476><loc_252><loc_481>2</page_footer>
<page_break>
<page_header><loc_88><loc_18><loc_145><loc_24>Work in Progress</page_header>
<text><loc_88><loc_54><loc_412><loc_122>scale corpora, it is impossible to rigorously determine whether a model solves a multi-hop query by reasoning over the context or simply by recalling a memorized shortcut connection from pretraining. To address this, we conduct a behavioral study using a controlled synthetic environment where the boundary between Parametric (internal) and Contextual (external) knowledge is strictly enforced (Allen-Zhu & Li, 2023a;b). Specifically, we construct a large-scale dataset of human biographies underpinned by a knowledge graph composed of extensive synthetic entities and relations with real-world meanings. This setup serves as a precise testbed for multi-hop factual task, allowing us to strictly decouple complementary reasoning capabilities into two atomic skills (Figure 1(a)): Parametric Reasoning (MEM) , which relies solely on internal knowledge encoded in model parameters, and Contextual Reasoning (CTX) , which relies solely on novel facts in the input context.</text>
<text><loc_88><loc_127><loc_412><loc_174>To rigorously assess the model's capability boundaries, we evaluate generalization under varying levels of difficulty, shown in Figure 1(b): I.I.D. (seen combinations), Composition (unseen combinations of seen relations), and Zero-shot (combinations involving unseen relations). Based on the strictly controlled knowledge with multi-hop QA pairs for MEM, CTX and COMP, we train models under different combinations and amounts of training data with varying post-training strategies ( e.g., SFT, RL). We evaluate the performance on the test set of COMP and study the optimal setting of training method and mix of training data for different levels of generalization.</text>
<text><loc_88><loc_179><loc_412><loc_226>Our experiments reveal a nuanced relationship between sufficient atomic skills and RL-driven generalization (Figure 1(c)). We demonstrate that simply (SFT) training on complementary data yields high in-distribution performance but fails to generalize to out-of-distribution scenarios with unseen combinations. However, RL serves as a catalyst for generalization under a strong condition that all atomic skills for the comprehensive tasks are captured, particularly in the challenging Zero-shot setting where relation combinations are entirely novel. Our findings challenge the view of RL as merely a probability amplifier. We summarize our key contributions as follows:</text>
<unordered_list><list_item><loc_88><loc_229><loc_412><loc_256>We formally define Complementary Reasoning and introduce a controlled dataset that decouples the complex ability into atomic Parametric and Contextual skills, enabling comprehensive evaluation of generalization levels ( i.e., I.I.D., Composition, and Zero-shot). We provide sufficient training data for each skills to facilitate a systematic investigation of training strategies.</list_item>
<list_item><loc_88><loc_261><loc_412><loc_280>Weprovide empirical evidence that while Supervised Fine-Tuning (SFT) is sufficient for memorizing distributions, Reinforcement Learning (RL) is essential for generalization, specifically enabling the model to tackle Zero-shot relational combinations that SFT fails to resolve.</list_item>
<list_item><loc_88><loc_285><loc_412><loc_312>We uncover a fundamental prerequisite for RL generalization: RL effectively composes new skills only when the base model possesses sufficient atomic capabilities . We show that a model SFT on atomic skills (SFTMEM+CTX ) gains significantly more from subsequent RL on complementary tasks than SFT directly on complementary data (SFTCOMP ) regardless of the amount of data.</list_item>
<list_item><loc_88><loc_317><loc_412><loc_336>We demonstrate that when reaching the prerequisite, RL does not merely amplify existing behaviors but actively composes learned atomic skills into new complex reasoning strategies, challenging the view that RL is solely a probability amplifier.</list_item>
</unordered_list>
<text><loc_88><loc_341><loc_412><loc_367>Our findings suggest a scalable path for training reasoning LLMs: rather than collecting expensive complex reasoning traces for extensive RL, one can focus on efficiently teaching the model sufficient fundamental atomic skills via SFT, and then leverage RL to unlock the generalization required for complex reasoning.</text>
<section_header_level_1><loc_88><loc_379><loc_173><loc_386>2 RELATED WORK</section_header_level_1>
<text><loc_88><loc_394><loc_412><loc_462>Roles of Post-training Strategies The power of LLMs is unleashed fundamentally through posttraining with an initial Supervised Fine-Tuning (SFT) stage and a following Reinforcement Learning (RL) stage. While SFT establishes stable output behavior and foundational knowledge, the maximumlikelihood estimation inherently favors memorization of the training distribution, leading to diminished robustness and poor generalization (Wang et al., 2025; Chu et al., 2025). In contrast, RL, driven by reward signals, transforms the model's distribution into goal-oriented, problem-solving strategies (Shao et al., 2024; Schulman et al., 2017). However, a hot debate recently is on whether RL combines new knowledge (Yuan et al., 2025; Liu et al., 2025), or just amplifying from the existing distribution (Wu & Choi, 2025; Yue et al., 2025; Yang et al.; Setlur et al., 2025). Our findings stand for RL incentivizes new skills, but only under some prerequisites.</text>
<page_footer><loc_248><loc_476><loc_252><loc_481>3</page_footer>
<page_break>
<page_header><loc_88><loc_18><loc_145><loc_24>Work in Progress</page_header>
<text><loc_88><loc_54><loc_412><loc_115>Knowledge-intensive Reasoning A foundational capability for intelligence is to reason with knowledge. Multi-Hop Reasoning, which requires multiple facts to answer, serves as an ideal testbed (Yang et al., 2018; Ho et al., 2020; Huang et al., 2025). Inherently, this task requires the ability to retrieve knowledge from parametric or external bases and logically compose intermediate facts (Huang et al., 2023b; Cheng et al., 2024b; Huang et al., 2024; Jin et al., 2025). Recent studies identified that LLMs struggle to generalize when using both new and known knowledge for reasoning (Cheng et al., 2024a; Yin et al., 2023). However, relying solely on recent benchmarks risks data contamination, making it hard to distinguish between new and known knowledge. This necessitates moving beyond standard benchmarks to controlled experimental setups.</text>
<text><loc_88><loc_124><loc_412><loc_192>Behavioral Study with Synthetic Data To address the fundamental challenges of data contamination and memory effects, synthetic data has become essential for LLMs evaluation (Allen-Zhu & Li, 2023a;b). With brand new facts, entities, or entire narratives (such as human biographies), researchers can strictly control the knowledge of the model, the sufficiency of knowledge for a question and the complexity of the task (Yin et al., 2023; Kim et al., 2025; Yuan et al., 2025; Wang et al., 2024). This also allows for rigorous evaluation of the model's capability under controlled settings. Our study adopts a relational knowledge base with templates to study knowledge-intensive reasoning under varying post-training strategies, where we are able to strictly control the parametric and new knowledge. With the synthetic relational patterns, we can systematically examine how LLMs could generalize to out-of-distribution ( O.O.D. ) patterns.</text>
<section_header_level_1><loc_88><loc_204><loc_333><loc_210>3 PROBLEM DEFINITION: COMPLEMENTARY REASONING</section_header_level_1>
<text><loc_88><loc_220><loc_412><loc_260>We formally define complementary reasoning, and introduce data construction and experiment settings. In real-world scenarios, LLMs can easily tackle multi-hop questions using either parametric or contextual knowledge, both of which are fundamental skills for complementary reasoning. However, acquiring ample data that encompasses both skills is challenging. We try to address these challenges: 1) scheduling of the post-training strategies to generalize beyond the training distribution; 2) recipe of mixing the training data for generalization on complementary reasoning.</text>
<text><loc_88><loc_266><loc_412><loc_299>Reasoning Types Complementary Reasoning , denoted as COMP , is defined as a multi-hop task requiring both parametric and contextual skills, which complements each other. In addition, we define Parametric and Contextual Reasoning as a task solely requires skills in LLMs' parameters or the context window, denoted as MEM and CTX , respectively. Formally, we define the capability requirement as a logical conjunction:</text>
<formula><loc_206><loc_305><loc_294><loc_311></formula>
<text><loc_88><loc_316><loc_412><loc_350>This implies that a failure in either atomic skill ( C MEM or C CTX ) necessitates a failure in the complementary task. Intuitively, humans find it straightforward to answer a complementary question if the new (unknown) skills are given. However, while capable of parametric and contextual reasoning by post-training with large-scale (Context-)Question-Answer examples, LLMs cannot generalize to complementary reasoning fluently (Cheng et al., 2024a).</text>
<text><loc_88><loc_355><loc_412><loc_409>In this paper, we study the generalization to complementary reasoning through multi-hop factual tasks. Typically, such question requires the retrieval of relevant knowledge and the capability to link several facts together. Formally, we define the task as traversing a relational path P = ( r 1 , r 2 , . . . , r k ) starting from a topic entity to reach the answer. For complementary reasoning, each relation r i in the path falls into either parametric or contextual knowledge, requiring the model to seamlessly integrate both sources. For the example question in Figure 1(a), ' What is the occupation of the business partner of Global View's new Chief Editor? ' traverses the path ' Global View Chief Editor - Business Partner - Occupation - Answer '.</text>
<text><loc_88><loc_415><loc_412><loc_462>Generalization Levels Traditional random data split assumes that testing data follows the same distribution as training data (Independent and Identically Distributed, a.k.a.. I.I.D.). However, this assumption fails to capture the complexity of real-world reasoning. For instance, a web agent often encounters infinite combinations of operations that cannot be exhaustively covered in a finite training set (Deng et al., 2023). To rigorously investigate whether models can transcend rote memorization and generalize to novel scenarios, we evaluate performance across three levels of difficulty based on the novelty of the relational path ( i.e., hops) in the question (Gu et al., 2021; Huang et al., 2023a).</text>
<page_footer><loc_248><loc_476><loc_252><loc_481>4</page_footer>
<page_break>
<page_header><loc_88><loc_18><loc_145><loc_24>Work in Progress</page_header>
<text><loc_88><loc_54><loc_412><loc_73>Given the relational path P defined above, let P train denote the set of relational paths seen during instruction tuning, and R train denote the set of individual atomic relations observed in those paths. We categorize the generalization levels of difficulty as follows (examples in in Figure 1(b)):</text>
<unordered_list><list_item><loc_88><loc_76><loc_412><loc_116>I.I.D. Generalization evaluates the ability to apply learned patterns to new entities within familiar structures. The relational path in the test question is fully observed during training ( P test ∈ P train ). For example, if the model is trained on questions involving the path Business Partner → Job ( e.g., 'What is the occupation of [X]'s business partner?'), an I.I.D. test query would ask the same question structure for a different entity. Our experiments (Section D) show that SFT is typically sufficient for this level as it merely requires recalling the observed path structure.</list_item>
<list_item><loc_88><loc_118><loc_412><loc_166>Compositional Generalization tests the model's ability to recombine in-distribution primitives into novel reasoning patterns. In concrete, the specific path is unseen ( P test / ∈ P train ), yet every individual relation constituting the path has been observed in disjoint contexts during training ( ∀ r ∈ P test , r ∈ R train ). For example, suppose the training set contains the paths Business Partner → Job and Business Partner → Spouse . A Composition test query might ask, 'What is the occupation of [X]'s spouse?' via the path Spouse → Job . Although the model knows both relations independently, it must synthesize them into a new compound reasoning path without explicit prior instruction.</list_item>
<list_item><loc_88><loc_168><loc_412><loc_215>Structural Zero-shot Generalization is the most challenging setting. Unlike standard zero-shot prompting, we define this as generalization to unseen relational primitives. Here, the relational path involves at least one relation that is never seen in any question-answer pair during training ( ∃ r ∈ P test , r / ∈ R train ). For example, if the relation Advisor exists in the knowledge base but never appears in the QA training set, asking 'Who is the spouse of [X]'s advisor?' requires the model to identify and utilize this latent knowledge solely from the context or parametric memory, without any supervised examples of how to query it.</list_item>
</unordered_list>
<section_header_level_1><loc_88><loc_228><loc_188><loc_234>4 EXPERIMENT SETUP</section_header_level_1>
<section_header_level_1><loc_88><loc_243><loc_229><loc_249>4.1 SYNTHETIC HUMAN BIOGRAPHIES</section_header_level_1>
<text><loc_88><loc_256><loc_412><loc_290>Using existing benchmarks presents two fundamental challenges: the risk of data contamination from the model's pre-training corpus and the difficulty in precisely controlling the parametric and contextual knowledge. To address these, we construct a controlled synthetic dataset of human biographies (Allen-Zhu & Li, 2023a;b), based on which we are able to control the sufficiency of knowledge. We build training and test data for the three reasoning types and generalization levels.</text>
<text><loc_88><loc_299><loc_412><loc_367>Synthetic Human Profiles with a Knowledge Graph We ground our dataset in a synthetic relational knowledge graph and transform the structured graph into natural language profiles with templates by an LLM (GPT-4o). We define 39 relations including eight symmetric relations ( e.g., spouse, sibling ), and eight pairs of inverse relations ( e.g., child and parent ) to mimic real-world complexity. For each relation, we adopt an LLM to construct and validate three natural language templates. We adopt heuristic rules to constrain the tailed entity of the relation ( e.g., birthday should be a date). We randomly partition these relations into two disjoint sets to simulate parametric and contextual knowledge sources, respectively. Using the Python Faker Library, we populate the graph with entities ( e.g., names, birthday ) strictly ensuring that character identities are novel and do not overlap with the model's pre-training corpus.</text>
<text><loc_88><loc_372><loc_412><loc_398>We automatically construct 10k biographies for MEM (simulating known facts in the model), and 10k for CTX (simulating new facts), with 5k shared characters to enable bridging between parameters and context (simulating real-world scenarios where new facts can be added to known characters). Refer to Appendix A for more details.</text>
<text><loc_88><loc_408><loc_412><loc_462>QA-Pair Construction from Relational Path To construct multi-hop questions, we sample relational paths/combinations consisting of 2-5 hops. Note that we ensure the number of hops for each reasoning type is distributed identically. We enforce a constraint where intermediate nodes must be named entities rather than generic values ( e.g., dates, emails) to ensure the multi-step dependency. For COMP, we guarantee that the paths incorporate relations from both MEM and CTX. Then, we employ an LLM to generate three linguistically diverse question templates to ensure syntactic variety without relying on repetitive patterns. Crucially, we incorporate Chain-of-Thought (CoT) (Wei et al., 2022) into the target answers for three strategic reasons: 1) to facilitate the decomposition of</text>
<page_footer><loc_248><loc_476><loc_252><loc_481>5</page_footer>
<page_break>
<page_header><loc_88><loc_18><loc_145><loc_24>Work in Progress</page_header>
<otsl><loc_87><loc_69><loc_410><loc_104><ched>Group<ched>Training<ched>I.I.D.<ched>Com.<ched>0-shot<ched>Training Data<ched>I.I.D. Com.<ched>0-shot<nl><rhed>Parametric<fcel>88,031<fcel>1,921<fcel>1,141<fcel>782<fcel>Parametric + Contextual<fcel>35.18 28.20<fcel>24.07<nl><rhed>Contextual<fcel>2,651<fcel>1,910<fcel>1,320<fcel>453<fcel>Complementary<fcel>90.30 76.25<fcel>18.41<nl><rhed>Complementary<fcel>180,919<fcel>2,135<fcel>1,415<fcel>918<ecel><ecel><ecel><nl><caption><loc_88><loc_52><loc_412><loc_65>Table 1: Statistics and empirical results based on generalization levels. Com. denotes Composition. (a) Statistics of training and testing set. (b) SFT results on Complementary Reasoning test set.</caption></otsl>
<text><loc_88><loc_117><loc_412><loc_150>complex multi-hop queries; 2) to facilitate retrieval and application of specific factual knowledge, as suggested by Allen-Zhu & Li (2023a); and 3) to enable precise evaluation of whether the model is retrieving intermediate facts from the parameter or contextual. To form the LLM's input, we directly query the model for MEM questions. For CTX and COMP, we present the contexts (knowledge documents-biographies) and the question, akin to real-world question answering systems.</text>
<text><loc_88><loc_165><loc_412><loc_185>Data Split for Generalization Levels To ensure the validity of the generalization levels (Section 3), we implement a rigorous multi-stage splitting protocol at the relational path level prior to natural language generation. Specifically:</text>
<text><loc_88><loc_190><loc_412><loc_216>1) For I.I.D. , we explicitly reserve a subset of relational paths, denoted as P iid . These paths are included in both the training and testing sets. To prevent rote memorization of answers, we ensure that while the relational paths are identical, the entities instantiated in the test set are disjoint from those in the training set.</text>
<text><loc_88><loc_221><loc_412><loc_248>2) For Zero-shot , from the remaining relational paths (excluding P iid ), we identify the full set of atomic relations and randomly sample a subset, R unseen , to be held out. We then filter the dataset: any path containing at least one relation r ∈ R unseen is exclusively assigned to the testing set. This guarantees that the model never encounters these specific relations during training.</text>
<text><loc_88><loc_253><loc_412><loc_279>3) For Composition , we randomly partition the remaining paths into two disjoint sets: P train comp (to the training set) and P test comp (to the testing set). Crucially, we verify that the set of atomic relations present in the test split is identical to that in the training split ( R test ≡ R train ), ensuring that the difficulty arises solely from the novel combination of known relations.</text>
<text><loc_88><loc_284><loc_412><loc_324>Through this protocol, we guarantee that I.I.D. tests seen paths, Composition tests unseen paths composed of seen relations, and Zero-shot tests paths with unseen relations. We then construct natural language question and CoT answer pairs by random-walk over the knowledge graph entities. For example, for a relation combination, we sample an entity as the starting point, and walk on the combination over the knowledge graph until we meet the answer. The final answer will then be appended after 'So, the answer is:', which is also used to filter the outcome for evaluation.</text>
<text><loc_88><loc_330><loc_412><loc_349>Since data generation is scalable, we provide an amount of training samples sufficient for excellent I.I.D. performance, with Supervised Fine-Tuning (SFT) training for Qwen-2.5-1.5B. Statistics of training and testing set are shown in Table 1a. Refer to Appendix A for more details.</text>
<section_header_level_1><loc_88><loc_365><loc_183><loc_371>4.2 TRAINING PROTOCOL</section_header_level_1>
<text><loc_88><loc_381><loc_412><loc_462>To systematically investigate the research questions (the necessary training strategies and the prerequisite data conditions), we adopt a standard post-training pipeline consisting of Supervised FineTuning (SFT) followed by Reinforcement Learning (RL). We employ Qwen-2.5-1.5B as our backbone model, as it is widely used in both recent methodologies and analytical works. As mentioned in Section 3, for parametric reasoning (MEM), the model receives only the Question as input. For Contextual (CTX) and Complementary Reasoning (COMP), the model receives both the Question and the Context (new facts). As for training, for SFT, we train models using standard next-token prediction. We also integrate all parametric knowledge documents (biographies) into the SFT process. For RL, we employ Group Relative Policy Optimization (GRPO) (Shao et al., 2024) with a binary outcome reward on the answer ( i.e. not on the reasoning chain). The outcome reward and the final evaluation are calculated by the exact match of the ground truth answer. In all experiments, the models are trained until convergence and evaluated on COMP test set.</text>
<page_footer><loc_248><loc_476><loc_252><loc_481>6</page_footer>
<page_break>
<page_header><loc_88><loc_18><loc_145><loc_24>Work in Progress</page_header>
<section_header_level_1><loc_88><loc_54><loc_335><loc_59>4.3 BASELINE ANALYSIS: THE LIMITS OF SUPERVISED FINE-TUNING</section_header_level_1>
<text><loc_88><loc_67><loc_412><loc_94>We first establish the baseline performance of SFT to demonstrate why supervised training alone is insufficient for robust Complementary Reasoning. We analyze the difficulty of the task and the generalization capabilities of two SFT baselines: a model trained on atomic skills (SFTMEM+CTX ) and a model trained directly on the target task (SFTCOMP ).</text>
<text><loc_88><loc_104><loc_412><loc_138>Complementary reasoning is data-hungry and inherently difficult. Table 1a reveals that achieving high in-distribution (I.I.D.) performance on COMP requires significantly more data than atomic tasks (MEM and CTX). The COMP task requires ∼ 180k samples-nearly double the sum of samples required for MEM ( ∼ 88k) and CTX ( ∼ 3k) combined. This validates that integrating internal and external knowledge is structurally more complex than applying either skill in isolation.</text>
<text><loc_88><loc_148><loc_412><loc_195>Atomic skills do not spontaneously compose. A key question is whether teaching a model the necessary components (SFT with MEM and CTX) is sufficient to infer the composite skill. Table 1b shows that SFTMEM+CTX achieves only approximately 24.07-35.18% performance on the COMP test set. While this is non-zero (indicating some transfer), it lags far behind the explicit training baseline (90.26% for I.I.D.), echoing findings in Cheng et al., 2024a; Yin et al., 2023. This suggests that simply possessing the knowledge and retrieval skills does not guarantee the ability to perform complex integration without further guidance. Refer to more analysis in Appendix E.1.</text>
<text><loc_88><loc_206><loc_412><loc_246>SFT Memorizes rather than Generalizes. Another critical finding is the failure of SFTCOMP in out-of-distribution (Zero-shot) settings. While COMP achieves a near-perfect 90.26% on I.I.D., its performance collapses to 26.25% on Zero-shot (Table 1b). This stark contrast indicates that SFT primarily incentivizes the model to memorize specific relational patterns seen during training. When tailored with novel relations where memorization is impossible, the superficial reasoning learned via SFT fails completely.</text>
<section_header_level_1><loc_88><loc_260><loc_355><loc_275>5 REINFORCEMENT LEARNING ENABLES GENERALIZATION IN COMPLEMENTARY REASONING</section_header_level_1>
<text><loc_88><loc_285><loc_412><loc_339>We have established that SFT alone-whether on atomic skills or target tasks-fails to generalize to complementary reasoning, especially on Zero-shot setting. We hypothesize that generalization is unlocked only through a specific curriculum: establishing sufficient atomic capabilities via SFT, followed by synthesizing these skills via RL . Formally, we propose that the recipe 'SFTMEM+CTX → RLCOMP' is the optimal path to generalization. In this section, we validate the Sufficiency and Necessity of our finding. We first show that this recipe consistently outperforms other data combinations regardless of data scale. Then, we demonstrate that sufficient atomic foundations is strictly required, and that 'SFT → RL' is the necessary training strategy for generalization.</text>
<section_header_level_1><loc_88><loc_351><loc_263><loc_356>5.1 SUFFICIENCY: RL AS A SKILL SYNTHESIZER</section_header_level_1>
<text><loc_88><loc_364><loc_412><loc_383>To rigorously test whether our proposed recipe 'SFTMEM+CTX → RLCOMP' consistently yields superior generalization, compared to direct training on the target task, we conduct a controlled experiment by varying the proportion of COMP data. Refer to more comparisons in Appendix D.</text>
<text><loc_88><loc_394><loc_412><loc_462>Settings. A common practice for complex tasks is to SFT as a cold-start and then RL over extensive training data (Guo et al., 2025). However, it is unclear how much data is needed to SFT for RL generalization for COMP. To address this, we partition the COMP training data into two subsets: a portion x % used for the SFT stage of the baseline, and the remaining (100 -x )% used for the RL stage. We compare two strategies using strictly identical RL data: 1) SFTCOMP → RLCOMP : SFT on x % COMP data and then RL on the remaining (100 -x )% ; 2) SFTMEM+CTX → RLCOMP : SFT on all atomic MEM + CTX training data (about 50% of COMP data in amount) and then RL on the same (100 -x )% COMP data. We vary x % from 10% to 90%. Figure 2 illustrates both the relative performance gain on test set of COMP derived specifically from the RL stage (Top Row) and the final absolute performance (Bottom Row).</text>
<page_footer><loc_248><loc_476><loc_252><loc_481>7</page_footer>
<page_break>
<page_header><loc_88><loc_18><loc_145><loc_24>Work in Progress</page_header>
<text><loc_361><loc_138><loc_370><loc_138>Comno</text>
<picture><loc_89><loc_52><loc_412><loc_145><caption><loc_88><loc_153><loc_412><loc_180>Figure 2: Comparison of reinforcement learning on different base models across complementary data proportions. The top row compares the gain from RL, while the bottom row compares the absolute performance after RL. It shows that LLMs generalize to Complementary Reasoning only from SFTed model over both Parametric and Contextual Reasoning.</caption></picture>
<text><loc_88><loc_188><loc_412><loc_235>RL efficiently synthesizes atomic skills. The top row of Figure 2 reveals a striking contrast in learning potential. Regardless of the volume of data allocated (varying from 10% to 90%), SFTMEM+CTX (red bars) achieves massive performance gains from RL with COMP data across all generalization levels. In contrast, SFTCOMP (green bars) shows diminishing returns, gaining almost nothing from RL. This indicates that without atomic foundations, RL cannot effectively explore the reasoning paths, merely optimizing within an existing distribution. Conversely, given atomic skills, RL acts as a powerful synthesizer, actively composing known facts into new reasoning paths.</text>
<text><loc_88><loc_247><loc_412><loc_287>Zero-shot generalization of RL over atomic skills is consistently superior. The most critical and challenging test of reasoning is the Zero-shot setting (Bottom Row, 3rd Column), where memorization is impossible. Here, SFTMEM+CTX (yellow bars) consistently and significantly outperforms SFTCOMP (blue bars) at every data scale. This confirms that our recipe fosters genuine generalization to unseen relations, whereas direct training on complementary data fails to extrapolate beyond the training distribution.</text>
<text><loc_88><loc_299><loc_412><loc_346>The SFT Generalization Paradox: Distinguishing Generalization from Memorization. We observe a nuanced result in the I.I.D. and Composition settings (Bottom Row, 1st & 2nd Columns). As the data portion x exceeds 70%, the absolute performance of SFTCOMP (blue bars) begins to surpass SFTMEM+CTX (yellow bars) on I.I.D. tasks, but fails on Zero-shot. We term this the memorization trap : SFT on composite data encourages the model to cache the specific shortcuts of the training data rather than learning the reasoning algorithm. RL on top of atomic skills avoids this trap by forcing the model to derive the path dynamically, resulting in robust Zero-shot performance.</text>
<section_header_level_1><loc_88><loc_359><loc_357><loc_364>5.2 NECESSITY: ATOMIC SKILLS AS PREREQUISITES FOR GENERALIZATION</section_header_level_1>
<text><loc_88><loc_372><loc_412><loc_406>While Section 5.1 establishes that 'SFTMEM+CTX → RLCOMP' is sufficient for generalization, distinct research questions remain regarding the boundary conditions: 1) Are both parametric and contextual skills strictly necessary, or can the model generalize from partial atomic skills? 2) Is RL specifically required, or can other training strategies ( e.g., further SFT) achieve similar results given the same atomic skills?</text>
<text><loc_88><loc_415><loc_412><loc_462>Necessity of Sufficient Atomic Skills (Data Condition). To determine if complete atomic capabilities is a prerequisite for RL-driven generalization, we compare SFTMEM+CTX against three baselines with similar initial performance but deficient atomic foundations: 1) SFTMEM (SFT with MEM only); 2) SFTCTX (SFT with CTX only); 3) SFT 10% COMP and SFT 20% COMP (SFT with COMP data reaching comparable initial performance). We apply identical RL training (using a fixed subset of 12.8k randomly sampled COMP data) to all models. Figure 3 presents the performance on COMP test set before and after RL.</text>
<page_footer><loc_248><loc_476><loc_252><loc_481>8</page_footer>
<page_break>
<page_header><loc_88><loc_18><loc_145><loc_24>Work in Progress</page_header>
<picture><loc_89><loc_53><loc_411><loc_115><caption><loc_88><loc_118><loc_412><loc_131>Figure 3: Necessity of atomic skills for RL generalization. We conduct RL with the same amount of COMP data from different SFT trained models. Only SFTMEM+CTX generalizes well in all levels.</caption></picture>
<text><loc_88><loc_136><loc_412><loc_211>Weobserve two critical findings. First, removing any atomic skill collapses generalization. Models SFTed solely on COMP or CTX fail to generalize significantly after RL, demonstrating that Complementary Reasoning is not merely an additive task but a synthesis requiring the sufficiency of atomic skills. Second, generalization potential is driven by foundational capability, rather than initial metric performance. Notably, SFT 10% COMP and SFT 20% COMP exhibit an initial performance similar to SFTMEM+CTX. However, after RL, their performance gain in all settings is negligible. In contrast, SFTMEM+CTX -equipped with the capability of both memory and context processing-achieves substantial gains from RL, nearly doubling the performance in all settings. Also, SFTMEM and SFTCTX, even with very poor initial performance, gain more from RL than the model SFTed with COMP data. This shows that initial test scores before RL are deceptive; the presence of underlying atomic skills is the true predictor of RL success.</text>
<text><loc_88><loc_217><loc_261><loc_258>Necessity of Reinforcement Learning for Zeroshot generalization (Training Strategy). Given a model with sufficient atomic skills (SFTMEM+CTX ), is RL strictly necessary to synthesize them? We compare three training strategies using the same 12.8k COMP samples: SFT, LoRA (rank=256), and RL.</text>
<text><loc_88><loc_263><loc_261><loc_303>Figure 4 shows that all training strategies can significantly improve the performance of the model with sufficient atomic skills, demonstrating its strong potential for generalization. However, it reveals a dichotomy between memorization and generalization. While further SFT (blue bar) yields the highest per-</text>
<picture><loc_271><loc_215><loc_409><loc_285></picture>
<text><loc_327><loc_283><loc_367><loc_287>Generalization Type</text>
<caption><loc_269><loc_287><loc_412><loc_299>Figure 4: Performance of training with different strategies over 12.8k COMP samples.</caption>
<text><loc_88><loc_304><loc_412><loc_331>formance on I.I.D. (indicating strong memorization of seen patterns), it significantly lags behind RL (green bar) in the Zero-shot setting. RL outperforms both SFT and LoRA in unseen relational combinations, confirming that while SFT is superior for pattern matching, RL is uniquely necessary to incentivize the active composition of skills required for out-of-distribution reasoning .</text>
<section_header_level_1><loc_88><loc_342><loc_371><loc_349>6 ANALYSIS OF THE MODEL WITH SUFFICIENT ATOMIC ABILITIES</section_header_level_1>
<text><loc_88><loc_358><loc_412><loc_391>Having established that capturing both the MEM and CTX abilities is the prerequisite for RL-driven generalization, we now analyze the features of base model for RL ( i.e., SFTMEM+CTX and SFTCOMP). Weinvestigate from the perspective of sample efficiency and the pass@k performance to see whether RL can compose new skills. We also showcase the training dynamics, the impact of training loss, a PCA analysis of latent space and uncertainty analysis in Appendix E.</text>
<section_header_level_1><loc_88><loc_401><loc_230><loc_407>6.1 SAMPLE EFFICIENCY COMPARISON</section_header_level_1>
<text><loc_88><loc_414><loc_412><loc_440>We analyze the properties of SFTMEM+CTX by examining whether learning atomic skills induces better sample efficiency compared to learning the composite task directly. We examine this from two perspectives: the data required to prime the base model (SFT efficiency) and the data required to unlock reasoning capabilities (adaptation efficiency).</text>
<text><loc_88><loc_450><loc_412><loc_462>Atomic capability learning requires less SFT data to prime RL generalization. To verify which training paradigm constructs a better foundation for RL with limited data, we conduct a</text>
<page_footer><loc_248><loc_476><loc_252><loc_481>9</page_footer>
<page_break>
<page_header><loc_88><loc_18><loc_145><loc_24>Work in Progress</page_header>
<picture><loc_89><loc_52><loc_412><loc_150><caption><loc_88><loc_153><loc_412><loc_166>Figure 5: Performance with the same amount of SFT and RL data. We compare SFTMEM+CTX and SFTCOMP by relative (the top row) and absolute performance (the bottom row) after RLCOMP .</caption></picture>
<text><loc_88><loc_172><loc_412><loc_219>controlled comparison. We first reserve a fixed, large-scale subset ( ∼ 90k) of COMP data exclusively for the RL stage. From the remaining data pool, i.e., same amount of (MEM + CTX) and COMP, we create subsets of increasing sizes (20%, 40%, 60%, 80%, 100%) to obtain SFTMEM+CTX and SFTCOMP. Crucially, to ensure a fair comparison, we control not just for the number of samples , but also for information content . We strictly enforce that the distribution of reasoning steps (hop counts) is the same across both training data, ensuring that the models are exposed to equivalent reasoning complexities. We then apply RL to all models using the same reserved COMP data.</text>
<text><loc_88><loc_224><loc_412><loc_278>Figure 5 presents the relative and absolute performance after RL on COMP test set comparing SFT and RL with the same amount of data. It shows that the model with sufficient atomic skills (SFTMEM+CTX, red/orange bars) consistently outperforms the composite model (SFTCOMP, green/blue bars) in both relative gain (the top row) and absolute performance (the bottom row) across all data scales. Even with minimal SFT data ( ∼ 18k), SFTMEM+CTX is successfully 'primed' for significant RL gains, whereas the SFTCOMP struggles to generalize. This suggests that training the model with sufficient atomic skills is a more data-efficient way to build the prerequisite representations for complex reasoning.</text>
<text><loc_88><loc_286><loc_242><loc_360>Sufficient atomic skills enable few-shot adaptation. Wefurther investigate how much COMP data is actually needed to 'trigger' generalization once sufficient atomic skills are established. Fixing SFTMEM+CTX as the base model, we perform further training using RL, SFT, or LoRA (rank=256) with varying sizes of COMP data, ranging from a 'tiny' set (50 samples) to a medium set (12,800 samples, < 10% of the total). We compare this against a skyline baseline: SFT on 100% COMP data (SFTCOMP).</text>
<text><loc_88><loc_366><loc_242><loc_392>Figure 6 shows the overall performance on COMP test set, from which we observe two key findings. First, rapid adaptation: Even with as few as 50-400 samples, SFTMEM+CTX adapts</text>
<picture><loc_252><loc_284><loc_409><loc_373></picture>
<text><loc_295><loc_370><loc_358><loc_374>Sample Size of Complementary Data</text>
<text><loc_366><loc_370><loc_378><loc_374>Scale)</text>
<text><loc_358><loc_370><loc_366><loc_374>(Log</text>
<caption><loc_250><loc_377><loc_411><loc_389>Figure 6: Few-shot adaptation of SFTMEM+CTX with different training strategies.</caption>
<text><loc_88><loc_393><loc_412><loc_426>to the complementary reasoning significantly better than the baseline start, regardless of the training strategy. Second, data efficiency: With less than 10% of COMP data, SFTMEM+CTX effectively matches the performance of the model SFT on the entire dataset (purple dotted line). This demonstrates that once atomic skills are acquired, the cost of 'assembling' them into a complex reasoning strategy is extremely low.</text>
<section_header_level_1><loc_88><loc_437><loc_277><loc_442>6.2 THE ROLE OF RL: SYNTHESIZER VS. AMPLIFIER</section_header_level_1>
<text><loc_88><loc_450><loc_412><loc_462>There is abundant debate in recent literature: Does RL incentivize the acquisition of genuinely new reasoning capabilities ( Synthesis ) , or merely re-weight the probability of existing skills already</text>
<page_footer><loc_246><loc_476><loc_254><loc_481>10</page_footer>
<page_break>
<page_header><loc_88><loc_18><loc_145><loc_24>Work in Progress</page_header>
<picture><loc_94><loc_53><loc_406><loc_127><caption><loc_88><loc_129><loc_412><loc_142>Figure 7: Pass@k comparison for SFTMEM+CTX and SFTCOMP. It shows that RL synthesizes new compositional skills only based on models with sufficient atomic skills.</caption></picture>
<text><loc_88><loc_150><loc_412><loc_218>present in the base distribution ( Amplification ) (Setlur et al., 2025)? To distinguish between these roles of RL, we analyze the pass@k performance on the COMP test set before and after RL (Yue et al., 2025). We vary k from 2 0 to 2 9 on a log scale. The logic is as follows: 1) Synthesis Signal: If a significant performance gap persists even at large k ( i.e., the curves remain parallel or divergent), it implies that RL has synthesized novel reasoning paths that the SFT model fundamentally cannot discover via sampling. 2) Amplification Signal: If the performance of the SFT model eventually catches up to the RL model as k increases ( i.e., the curves merge), it implies that the reasoning paths are latent in the SFT distribution, and RL merely amplifies their generation probability. We compare our SFTMEM+CTX against a strong baseline (SFTCOMP trained on 90% data) using identical RL data. Figure 7 presents the results over different generalization levels.</text>
<text><loc_88><loc_229><loc_412><loc_318>RL synthesizes new pathways for models with sufficient atomic skills. The top row of Figure 7 reveals a distinct parallel scaling law. For SFTMEM+CTX, the RL performance (orange line) remains significantly higher than the SFT baseline (blue line) even as k increases to 2 9 . The non-convergence of the pass@k curves suggests a fundamental shift in mechanism. If RL were merely amplifying latent behaviors, the SFT model (given enough attempts k ) would eventually find the solution. The persistent gap indicates that RL has synthesized a logic circuitspecifically, the mechanism to bridge context and memory -that is effectively absent from the SFT distribution. This provides empirical evidence that, given sufficient atomic priors, RL creates new capabilities rather than just optimizing old ones. This persistent gap serves as strong empirical evidence that RL is not just re-weighting existing skills. Instead, it is actively synthesizing atomic skills into novel reasoning strategies that are effectively non-existent in the base model's sampling distribution. Notably, this discovery of 'new' capabilities occurs not only for Zero-shot but also on I.I.D. and Compositional settings, suggesting that RL optimizes the logic of combination itself.</text>
<text><loc_88><loc_329><loc_412><loc_370>RL merely amplifies existing behaviors for models' composite skills. In stark contrast, the bottom row of Figure 7 shows that for SFTCOMP , the curves rapidly converge as k increases. By k = 2 9 , the SFT model nearly matches the RL model's performance. This indicates that because the model trained on composite data memorized the target distribution during SFT, RL serves primarily as an amplifier-boosting the likelihood of the 'best' answer without discovering fundamentally new reasoning paths.</text>
<text><loc_88><loc_375><loc_412><loc_387>This dichotomy confirms that sufficient atomic skills are the prerequisite for RL to function as a reasoning synthesizer. Without them, RL degenerates into a mere probability amplifier.</text>
<section_header_level_1><loc_88><loc_400><loc_153><loc_405>6.3 CASE STUDY</section_header_level_1>
<text><loc_88><loc_414><loc_412><loc_461>Having demonstrated quantitative evidence for SFTMEM+CTX being key to RL generalization. We now qualitatively analyze from specific error cases on the intersection of incorrect samples for the models before and after RL. By aligning the generated CoT with the ground truth, we pinpoint the exact reasoning step where the deviation occurred. We classify these errors based on the required knowledge-parametric (MEM) versus contextual (CTX)-and calculate the progress (the normalized position of the failure step within the reasoning path). Table 2 shows distinct error patterns among the models.</text>
<page_footer><loc_246><loc_476><loc_254><loc_481>11</page_footer>
<page_break>
<page_header><loc_88><loc_18><loc_145><loc_24>Work in Progress</page_header>
<text><loc_88><loc_54><loc_258><loc_128>SFTMEM+CTX , SFTCOMP , and SFTCOMP → RLCOMP exhibit similar failure modes, characterized by a high prevalence of CTX errors ( > 85% ) and earlystage failures (Progress ≤ 54 . 5% ). Qualitatively, SFTMEM+CTX tends to hallucinate when retrieving information from the provided context, similar to its imitation learning process ( i.e., contextual reasoning). Conversely, SFTCOMP variants frequently fail to identify the correct relation when bridging from MEM to CTX, with 62% of errors in SFTCOMP getting stuck at the first hop.</text>
<text><loc_266><loc_55><loc_412><loc_88>Table 2: Error analysis. MEM and CTX denote errors occurring at parametric and contextual relations, respectively. Prog. represents the normalized position of the first error step within the reasoning path.</text>
<otsl><loc_267><loc_90><loc_411><loc_126><ched>Model<ched>CTX<ched>MEM<ched>Prog.<nl><rhed>SFT COMP<fcel>90%<fcel>10%<fcel>54.5%<nl><rhed>SFT COMP → RL COMP<fcel>86%<fcel>14%<fcel>45.0%<nl><rhed>SFT MEM+CTX<fcel>86%<fcel>14%<fcel>18.5%<nl><rhed>SFT MEM+CTX → RL COMP<fcel>30%<fcel>70%<fcel>71.8%<nl></otsl>
<text><loc_88><loc_134><loc_412><loc_188>In contrast, SFTMEM+CTX → RLCOMP demonstrates a fundamental shift in knowledge flow utilization. The error distribution flips significantly: 70% of errors are from MEM, and the average error position occurs much later (71.8%), mostly at the final hop. This indicates that RL effectively optimizes the reasoning path through the knowledge flow . We hypothesize that the vulnerability at the last step is attributed to data sparsity in complementary reasoning tasks. Since our data ( i.e., human biographies) relies on 'character' entities as bridges, the final relation connecting to the answer (which can be either a character or a keyword) appears less frequently in the training distribution compared to the intermediate bridging relations.</text>
<section_header_level_1><loc_88><loc_200><loc_164><loc_206>7 CONCLUSIONS</section_header_level_1>
<text><loc_88><loc_216><loc_412><loc_290>While current LLMs are proficient in multi-hop reasoning with either parametric or contextual knowledge, they struggle to handle questions that require the integration of both knowledge sources. In this paper, we study how LLMs can generalize to Complementary Reasoning with post-training strategies ( i.e., SFT and RL). We conduct strictly controlled experiments with our synthetic human biographies based on a relational knowledge graph, constructing QA pairs for atomic skills ( i.e., Parametric and Contextual Reasoning) and the compound skill ( i.e., Complementary Reasoning). We also split the dataset into different levels of generalization difficulties I.I.D., Composition and Zero-shot. Based on this, we study the effect of training strategies and find that only by firstly SFT with sufficient atomic skills can LLMs generalize via RL. This finding suggests a scalable path for training reasoning LLMs: focusing on teaching the model sufficient fundamental atomic skills via SFT, and then leveraging RL for generalization to O.O.D. complex tasks.</text>
<section_header_level_1><loc_88><loc_303><loc_143><loc_309>LIMITATIONS</section_header_level_1>
<text><loc_88><loc_318><loc_412><loc_428>Although we have validated our conclusion that only 'SFTMEM+CTX → RLCOMP' generalizes, which well addresses the research questions, we still have some limitations: 1) Although we show from several distinct angles that our finding holds, we only test on Qwen models. While evaluated on the Qwen family, our decomposition of reasoning into atomic priors suggests these findings are likely architecture-agnostic. 2) As we investigate from the sample efficiency, the pass@k performance, the training dynamic, the embedding distributions and case study, it is still difficult to find the mechanisms that enable the SFTMEM+CTX model to outperform. Future work should investigate the mechanistic interpretability of how RL circuits recruit atomic attention heads. 3) While our study relies on synthetic biographies, this was a necessary design choice to strictly enforce the boundary between Parametric (known) and Contextual (new) information-a boundary that is impossible to guarantee in web-scale corpora due to pre-training contamination. Future work should validate these findings on controlled splits of real-time benchmarks ( e.g., news QA) where the 'new' information is strictly dated post-training. Moreover, future work should study how to mix COMP data with MEM and CTX data to improve the overall I.I.D., Composition and Zero-shot performance. 4) It would be interesting to adapt our findings into real-world knowledge-intensive benchmarks, especially to check the O.O.D. testing performance.</text>
<section_header_level_1><loc_88><loc_438><loc_163><loc_443>ACKNOWLEDGMENTS</section_header_level_1>
<text><loc_88><loc_450><loc_412><loc_462>The authors would like to thank Shuyan Zhou, Weixi Feng, Yiheng Shu, Yunzhi Yao, Tengxiao Liu, Fangkai Yang, Antonis Antoniades, Heng Lin and Jiajun He for their valuable feedback.</text>
<page_footer><loc_246><loc_476><loc_254><loc_481>12</page_footer>
<page_break>
<page_header><loc_88><loc_18><loc_145><loc_24>Work in Progress</page_header>
<section_header_level_1><loc_88><loc_53><loc_143><loc_60>REFERENCES</section_header_level_1>
<unordered_list><list_item><loc_88><loc_65><loc_412><loc_84>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 , 2023.</list_item>
<list_item><loc_88><loc_91><loc_412><loc_103>Zeyuan Allen-Zhu and Yuanzhi Li. Physics of language models: Part 3.1, knowledge storage and extraction. arXiv preprint arXiv:2309.14316 , 2023a.</list_item>
<list_item><loc_88><loc_109><loc_412><loc_122>Zeyuan Allen-Zhu and Yuanzhi Li. Physics of language models: Part 3.2, knowledge manipulation. arXiv preprint arXiv:2309.14402 , 2023b.</list_item>
<list_item><loc_88><loc_128><loc_412><loc_154>Kaikai An, Fangkai Yang, Liqun Li, Junting Lu, Sitao Cheng, Shuzheng Si, Lu Wang, Pu Zhao, Lele Cao, Qingwei Lin, et al. Thread: A logic-based data organization paradigm for how-to question answering with retrieval augmented generation. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing , pp. 18300-18319, 2025.</list_item>
<list_item><loc_88><loc_161><loc_412><loc_180>Sitao Cheng, Liangming Pan, Xunjian Yin, Xinyi Wang, and William Yang Wang. Understanding the interplay between parametric and contextual knowledge for large language models. arXiv preprint arXiv:2410.08414 , 2024a.</list_item>
<list_item><loc_88><loc_186><loc_412><loc_206>Sitao Cheng, Ziyuan Zhuang, Yong Xu, Fangkai Yang, Chaoyun Zhang, Xiaoting Qin, Xiang Huang, Ling Chen, Qingwei Lin, Dongmei Zhang, et al. Call me when necessary: Llms can efficiently and faithfully reason over structured environments. arXiv preprint arXiv:2403.08593 , 2024b.</list_item>
<list_item><loc_88><loc_212><loc_412><loc_231>Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc V Le, Sergey Levine, and Yi Ma. Sft memorizes, rl generalizes: A comparative study of foundation model post-training. arXiv preprint arXiv:2501.17161 , 2025.</list_item>
<list_item><loc_88><loc_238><loc_412><loc_257>Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards a generalist agent for the web. Advances in Neural Information Processing Systems , 36:28091-28114, 2023.</list_item>
<list_item><loc_88><loc_263><loc_412><loc_283>Yu Gu, Sue Kase, Michelle Vanni, Brian Sadler, Percy Liang, Xifeng Yan, and Yu Su. Beyond iid: three levels of generalization for question answering on knowledge bases. In Proceedings of the web conference 2021 , pp. 3477-3488, 2021.</list_item>
<list_item><loc_88><loc_289><loc_412><loc_308>Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948 , 2025.</list_item>
<list_item><loc_88><loc_315><loc_412><loc_334>Bernal Jim´ enez Guti´ errez, Yiheng Shu, Weijian Qi, Sizhe Zhou, and Yu Su. From rag to memory: Non-parametric continual learning for large language models. arXiv preprint arXiv:2502.14802 , 2025.</list_item>
<list_item><loc_88><loc_340><loc_412><loc_359>Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing a multi-hop qa dataset for comprehensive evaluation of reasoning steps. arXiv preprint arXiv:2011.01060 , 2020.</list_item>
<list_item><loc_88><loc_366><loc_412><loc_378>Xiang Huang, Sitao Cheng, Yuheng Bao, Shanshan Huang, and Yuzhong Qu. Markqa: A large scale kbqa dataset with numerical reasoning. arXiv preprint arXiv:2310.15517 , 2023a.</list_item>
<list_item><loc_88><loc_385><loc_412><loc_404>Xiang Huang, Sitao Cheng, Yiheng Shu, Yuheng Bao, and Yuzhong Qu. Question decomposition tree for answering complex questions over knowledge bases. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 37, pp. 12924-12932, 2023b.</list_item>
<list_item><loc_88><loc_410><loc_412><loc_430>Xiang Huang, Sitao Cheng, Shanshan Huang, Jiayu Shen, Yong Xu, Chaoyun Zhang, and Yuzhong Qu. Queryagent: A reliable and efficient reasoning framework with environmental feedbackbased self-correction. arXiv preprint arXiv:2403.11886 , 2024.</list_item>
<list_item><loc_88><loc_436><loc_412><loc_462>Xiang Huang, Jiayu Shen, Shanshan Huang, Sitao Cheng, Xiaxia Wang, and Yuzhong Qu. Targa: Targeted synthetic data generation for practical reasoning over structured data. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pp. 2704-2726, 2025.</list_item>
</unordered_list>
<page_footer><loc_246><loc_476><loc_254><loc_481>13</page_footer>
<page_break>
<page_header><loc_88><loc_18><loc_145><loc_24>Work in Progress</page_header>
<unordered_list><list_item><loc_88><loc_54><loc_412><loc_80>Mingyu Jin, Weidi Luo, Sitao Cheng, Xinyi Wang, Wenyue Hua, Ruixiang Tang, William Yang Wang, and Yongfeng Zhang. Disentangling memory and reasoning ability in large language models. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pp. 1681-1701, 2025.</list_item>
<list_item><loc_88><loc_87><loc_412><loc_106>Minsung Kim, Dong-Kyum Kim, Jea Kwon, Nakyeong Yang, Kyomin Jung, and Meeyoung Cha. Training dynamics of parametric and in-context knowledge utilization in language models. arXiv preprint arXiv:2510.02370 , 2025.</list_item>
<list_item><loc_88><loc_114><loc_412><loc_133>Mingjie Liu, Shizhe Diao, Ximing Lu, Jian Hu, Xin Dong, Yejin Choi, Jan Kautz, and Yi Dong. Prorl: Prolonged reinforcement learning expands reasoning boundaries in large language models. arXiv preprint arXiv:2505.24864 , 2025.</list_item>
<list_item><loc_88><loc_140><loc_412><loc_166>Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. When not to trust language models: Investigating effectiveness of parametric and non-parametric memories. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pp. 9802-9822, 2023.</list_item>
<list_item><loc_88><loc_173><loc_412><loc_186>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347 , 2017.</list_item>
<list_item><loc_88><loc_193><loc_412><loc_212>Amrith Setlur, Matthew Y. R. Yang, Charlie Snell, Jeremy Greer, Ian Wu, Virginia Smith, Max Simchowitz, and Aviral Kumar. e3: Learning to explore enables extrapolation of test-time compute for llms, 2025. URL https://arxiv.org/abs/2506.09026 .</list_item>
<list_item><loc_88><loc_219><loc_412><loc_238>Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300 , 2024.</list_item>
<list_item><loc_88><loc_246><loc_412><loc_265>Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530 , 2024.</list_item>
<list_item><loc_88><loc_272><loc_412><loc_284>Boshi Wang, Xiang Yue, Yu Su, and Huan Sun. Grokked transformers are implicit reasoners: A mechanistic journey to the edge of generalization. arXiv preprint arXiv:2405.15071 , 2024.</list_item>
<list_item><loc_88><loc_291><loc_412><loc_318>Xinyi Wang, Antonis Antoniades, Yanai Elazar, Alfonso Amayuelas, Alon Albalak, Kexun Zhang, and William Yang Wang. Generalization v.s. memorization: Tracing language models' capabilities back to pretraining data. In The Thirteenth International Conference on Learning Representations , 2025. URL https://openreview.net/forum?id=IQxBDLmVpT .</list_item>
<list_item><loc_88><loc_325><loc_412><loc_344>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems , 35:24824-24837, 2022.</list_item>
<list_item><loc_88><loc_351><loc_412><loc_363>Fang Wu and Yejin Choi. The invisible leash: Why rlvr may not escape its origin. In 2nd AI for Math Workshop@ ICML 2025 , 2025.</list_item>
<list_item><loc_88><loc_371><loc_412><loc_383>Shiming Yang, Yuxuan Tong, Xinyao Niu, Graham Neubig, and Xiang Yue. Demystifying long chain-of-thought reasoning. In Forty-second International Conference on Machine Learning .</list_item>
<list_item><loc_88><loc_390><loc_412><loc_416>Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 conference on empirical methods in natural language processing , pp. 2369-2380, 2018.</list_item>
<list_item><loc_88><loc_423><loc_412><loc_436>Xunjian Yin, Baizhou Huang, and Xiaojun Wan. Alcuna: Large language models meet new knowledge. arXiv preprint arXiv:2310.14820 , 2023.</list_item>
<list_item><loc_88><loc_443><loc_412><loc_462>Lifan Yuan, Weize Chen, Yuchen Zhang, Ganqu Cui, Hanbin Wang, Ziming You, Ning Ding, Zhiyuan Liu, Maosong Sun, and Hao Peng. From f ( x ) and g ( x ) to f ( g ( x )) : Llms learn new skills in rl by composing old ones, 2025. URL https://arxiv.org/abs/2509.25123 .</list_item>
</unordered_list>
<page_footer><loc_246><loc_476><loc_254><loc_481>14</page_footer>
<page_break>
<page_header><loc_88><loc_18><loc_145><loc_24>Work in Progress</page_header>
<unordered_list><list_item><loc_88><loc_54><loc_412><loc_73>Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Shiji Song, and Gao Huang. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? arXiv preprint arXiv:2504.13837 , 2025.</list_item>
<list_item><loc_88><loc_80><loc_412><loc_127>Ziyuan Zhuang, Zhiyang Zhang, Sitao Cheng, Fangkai Yang, Jia Liu, Shujian Huang, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang, and Qi Zhang. EfficientRAG: Efficient retriever for multi-hop question answering. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing , pp. 3392-3411, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main.199. URL https://aclanthology.org/2024. emnlp-main.199/ .</list_item>
</unordered_list>
<page_footer><loc_246><loc_476><loc_254><loc_481>15</page_footer>
<page_break>
<page_header><loc_88><loc_18><loc_145><loc_24>Work in Progress</page_header>
<otsl><loc_91><loc_64><loc_409><loc_340><ched>Relation<ched>Statement Template<ched>Question Template<nl><fcel>address awards best friend birth date birth place boss boss of child classmate colleague died in died on email favorite food first language hobby influence influenced by known for leader of lived in major mentored by mentoring nationality neighbor occupation parent pet philanthropy phone rival roommate service sibling spouse university worked at wrote<fcel>{ e1 } resides at { e2 } . { e1 } won the { e2 } award. { e1 } 's best friend is { e2 } . { e1 } was born on { e2 } . { e1 } hails from { e2 } . { e1 } works under { e2 } . { e1 } manages { e2 } . { e2 } is the child of { e1 } . { e1 } studied alongside { e2 } . { e1 } works alongside { e2 } . { e1 } passed away in { e2 } . { e1 } died on { e2 } . You can reach { e1 } at { e2 } . { e1 } 's favorite dish was { e2 } . { e1 } 's native language was { e2 } . A favorite activity of { e1 } is { e2 } . { e1 } shaped the career of { e2 } . { e1 } looked up to { e2 } . { e1 } gained recognition for { e2 } . { e1 } was the leader of { e2 } . { e1 } resided in { e2 } . { e1 } majored in { e2 } . { e1 } received guidance from { e2 } . { e2 } is a student of { e1 } . { e1 } is a citizen of { e2 } . { e1 } lives next to { e2 } . { e1 } is employed as { e2 } . { e1 } 's parent is { e2 } . { e1 } owns a pet called { e2 } . { e1 } donated to { e2 } . { e1 } can be reached at { e2 } . { e1 } had a rivalry with { e2 } . { e1 } shared a room with { e2 } . { e1 } was a member of { e2 } . { e1 } and { e2 } are siblings. { e1 } is married to { e2 } . { e1 } went to { e2 } . { e1 } held a position at { e2 } . { e1 } authored the book { e2 } .<fcel>What is { e1 } 's address? What awards has { e1 } won? Who is { e1 } 's closest friend? When was { e1 } born? Where was { e1 } born? Who is { e1 } 's boss? Who works under { e1 } ? Who is { e1 } 's child? Who attended school with { e1 } ? Who are { e1 } 's colleagues? Where did { e1 } die? When did { e1 } pass away? What is { e1 } 's email address? What food does { e1 } enjoy the most? What is { e1 } 's first language? What does { e1 } enjoy doing? Who was influenced by { e1 } ? Who inspired { e1 } ? What is { e1 } famous for? Which group was { e1 } in charge of? Where has { e1 } lived? What did { e1 } specialize in? Who mentored { e1 } ? Who does { e1 } mentor? What is { e1 } 's nationality? Who resides beside { e1 } ? What does { e1 } do for a living? Who is the parent of { e1 } ? What is the name of { e1 } 's pet? Which causes did { e1 } support? What is { e1 } 's phone number? Who did { e1 } compete with? Who lived with { e1 } ? Which organization did { e1 } serve in? Who are { e1 } 's siblings? Who is { e1 } 's spouse? Which university did { e1 } attend? Where did { e1 } work? Which book did { e1 } write?<nl><caption><loc_154><loc_52><loc_346><loc_58>Table 3: Templates for Relations in our Knowledge Graph.</caption></otsl>
<section_header_level_1><loc_88><loc_360><loc_250><loc_367>A DETAILS OF DATA CONSTRUCTION</section_header_level_1>
<text><loc_88><loc_377><loc_412><loc_390>To systematically construct our synthetic human biography, we build a relational knowledge graph with fake information by Python Faker Library 1 and the help of GPT-4o.</text>
<text><loc_88><loc_395><loc_412><loc_428>We finally synthesize 39 relations including eight symmetric relations ( e.g., spouse, sibling ), and eight pairs of inverse relations ( e.g., child and parent ) to mimic real-world complexity. For each relation, we adopt an LLM to construct and validate three natural language templates. We adopt heuristic rules to constrain the tailed entity of the relation ( e.g., birthday should be a date). Table 3 shows the relations and templates.</text>
<text><loc_88><loc_433><loc_412><loc_446>For QA pairs construction, after sampling the relation paths/combinations, we adopt GPT-4o to generate question templates. Table 4 shows an example question template.</text>
<footnote><loc_99><loc_456><loc_213><loc_462>1 https://faker.readthedocs.io/en/master/</footnote>
<page_footer><loc_246><loc_476><loc_254><loc_481>16</page_footer>
<page_break>
<page_header><loc_88><loc_18><loc_145><loc_24>Work in Progress</page_header>
<otsl><loc_88><loc_71><loc_413><loc_190><ched>Component<ched>Content / Examples<nl><fcel>Relation Path<fcel>sibling - → boss of - → mentored by - → best friend<nl><fcel>Question Templates<fcel>1. Who is the best friend of the person mentoring the employee of the sibling of { e1 } ? 2. Can you tell me the best friend of the person who mentored the employee of { e1 } 's sibling? 3. Who is the best friend of the person mentoring the employee of the sibling that { e1 } has?<nl><fcel>CoT Answer Templates<fcel>1. { e2 } is { e1 } 's brother/sister. { e3 } works under { e2 } . { e3 } was trained by { e4 } . { e4 } 's best friend is { e5 } . So, the answer is: { e5 } 2. { e1 } and { e2 } are siblings. { e2 } is the boss of { e3 } . { e3 } was trained by { e4 } . { e4 } 's best friend is { e5 } . So, the answer is: { e5 } 3. { e2 } is { e1 } 's brother/sister. { e2 } is the boss of { e3 } . { e3 } received guid- ance from { e4 } . { e5 } is { e4 } 's closest friend. So, the answer is: { e5 }<nl><caption><loc_88><loc_52><loc_412><loc_65>Table 4: An example of Question and CoT answer templates based on a relation path (4-hop). { e1 } denotes the topic entity in the question.</caption></otsl>
<picture><loc_89><loc_201><loc_411><loc_267><caption><loc_95><loc_275><loc_405><loc_280>Figure 8: Model scaling analysis of Qwen model family across 0.5B, 1.5B, and 3B parameters.</caption></picture>
<text><loc_88><loc_296><loc_412><loc_330>After we split the relation combinations based on the generalization levels 3, we can synthesize human biographies based on the knowledge graph. We firstly conduct random walk over the knowledge graph entities, and then translate the obtained dict-formed biographies into natural language paragraphs with the relation templates. Table 5 shows an example of the biography dict and corresponding natural language paragraph.</text>
<section_header_level_1><loc_88><loc_343><loc_271><loc_349>B DETAILS OF SFT TRAINING BASELINES</section_header_level_1>
<text><loc_88><loc_359><loc_412><loc_399>As discussed in Section 4.1, we are able to synthesize as much data as needed. However, in realworld scenarios, while LLMs can easily handle either Parametric or Contextual Reasoning, probably through post-training with sufficient data, they struggles in Complementary Reasoning task, where it is hard to collect ample data. We show in Table 1a that Complementary Reasoning is data-hungry and most difficult compared to parametric and contextual reasoning. Here we show empirical results training with SFT and evaluating on the corresponding testing set in Table 6.</text>
<text><loc_88><loc_404><loc_412><loc_444>It further shows that Contextual Reasoning is the easiest. While LLMs learns to adopt new knowledge during SFT training, they manage to handle most of the unseen knowledge in the context window. We hypothesize that this ability is essential for further generalization to new reasoning patterns. Moreover, SFT training involving parametric knowledge would be hard to generalize on Zero-shot settings. Both parametric and complementary reasoning, while performing well on I.I.D. setting, drop significantly on Composition and almost fail on Zero-shot setting.</text>
<text><loc_88><loc_450><loc_412><loc_462>This further highlights our motivations: we try to figure out the recipe of training strategies and mix of training data required for generalization to complementary reasoning on all difficulty levels.</text>
<page_footer><loc_246><loc_476><loc_254><loc_481>17</page_footer>
<page_break>
<page_header><loc_88><loc_18><loc_145><loc_24>Work in Progress</page_header>
<otsl><loc_112><loc_71><loc_389><loc_283><ched>Python Dictionary<ched>Natural Language Document<nl><fcel>'Allison Hill': { 'name': 'Allison Hill', 'birth date': '1942-04-29', 'occupation': 'Civil engineer, consulting', 'email': 'garzaanthony@example.org', 'phone': '538.990.8386', 'new': true, 'died on': '2024-11-01', 'child': 'Donald Marsh', 'pet': 'Whiskers', 'wrote': 'Baby administration', 'influenced by': 'Matthew Cooper', 'mentoring': 'Daniel Watkins', 'hobby': 'painting', 'classmate': 'Adam Villanueva', 'first language': 'Finnish', 'roommate': 'Shannon Krause', 'university': 'University of Chicago', 'service': 'Habitat for Humanity', 'known for': 'painting', 'died in': 'Brownbury', 'boss': 'Lindsey Johnson', 'favorite food': 'tacos'<fcel>Allison Hill has a pet named Whiskers. Allison Hill spoke Finnish as their first language. A favorite activity of Alli- son Hill is painting. Lindsey Johnson is the boss of Allison Hill. Allison Hill was born on 1942-04-29. Allison Hill died on 2024-11-01. Allison Hill shared a room with Shannon Krause. Alli- son Hill penned Baby administration. Allison Hill was inspired by Matthew Cooper. The contact email for Alli- son Hill is garzaanthony@example.org. Allison Hill was famous for painting. Allison Hill was a member of Habi- tat for Humanity. Allison Hill mentors Daniel Watkins. Allison Hill's place of death was Brownbury. Allison Hill's phone number is 538.990.8386. Allison Hill works as a Civil engineer, consult- ing. Allison Hill is the parent of Don- ald Marsh. Allison Hill was a classmate of Adam Villanueva. Allison Hill loved eating tacos. Allison Hill went to Uni- versity of Chicago.<nl><caption><loc_88><loc_52><loc_412><loc_65>Table 5: An example of human biography in the form of Python Dict and Natural Language Document.</caption></otsl>
<otsl><loc_149><loc_318><loc_351><loc_353><ched>Training Data<ched>I.I.D<ched>Comp<ched>Zero-shot<nl><rhed>Parametric Reasoning<fcel>93.96<fcel>82.30<fcel>3.71<nl><rhed>Contextual Reasoning<fcel>98.53<fcel>95.53<fcel>69.53<nl><rhed>Complementary Reasoning<fcel>90.26<fcel>76.61<fcel>7.76<nl><caption><loc_88><loc_292><loc_412><loc_311>Table 6: Empirical performance. For each reasoning type, we training on the training set and test on the corresponding test set. Note that this is different from other experiments that focus on evaluation over complementary reasoning test set.</caption></otsl>
<section_header_level_1><loc_88><loc_368><loc_177><loc_374>C MODEL SCALING</section_header_level_1>
<text><loc_88><loc_384><loc_412><loc_410>We study whether our findings that 'RL enables generalization in Complementary Reasoning from sufficient atomic skills' persist as models scale. We compare Qwen-2.5-0.5B, Qwen-2.5-1.5B and Qwen-2.5-3B due to limited compute. Figure 8 illustrates the performance trends across model sizes. We observe consistent behaviors that strongly support the effectiveness of our proposed recipe.</text>
<text><loc_88><loc_415><loc_412><loc_462>As shown by the blue lines, SFTMEM+CTX leads to substantial performance jumps after RLCOMP . This improvement is particularly pronounced in the Zero-shot setting, indicating that the model with sufficient atomic skills successfully generalizes the reasoning capabilities acquired during RL. In contrast, SFTCOMP (orange dashed line), while starting with decent performance, exhibits limited growth after RL training (orange solid line). The gap between the pre-RL and post-RL performance is marginal, suggesting that SFT with composite data may limit the model's potential for further generalization.</text>
<page_footer><loc_246><loc_476><loc_254><loc_481>18</page_footer>
<page_break>
<page_header><loc_88><loc_18><loc_145><loc_24>Work in Progress</page_header>
<otsl><loc_129><loc_64><loc_371><loc_114><ched>Setting<ched>I.I.D<ched>Comp<ched>Zero-shot<nl><rhed>SFT MEM+CTX<fcel>35.18<fcel>28.20<fcel>24.07<nl><rhed>SFT MEM+CTX → RL MEM+CTX<fcel>28.43<fcel>28.34<fcel>27.89<nl><rhed>SFT MEM+CTX → RL MEM+CTX → RL COMP<fcel>74.00<fcel>62.47<fcel>49.56<nl><rhed>SFT MEM + CTX + COMP<fcel>80.14<fcel>62.90<fcel>43.25<nl><rhed>SFT MEM+CTX → RL COMP<fcel>73.11<fcel>60.85<fcel>50.87<nl><caption><loc_144><loc_52><loc_356><loc_58>Table 7: Empirical results on Complementary Reasoning test set.</caption></otsl>
<text><loc_88><loc_129><loc_412><loc_162>Crucially, these trends hold true across all model sizes . When the model size increases to 3B, the superiority of SFTMEM+CTX → RLCOMP over SFTCOMP → RLCOMP remains significant ( e.g., a gap of approximately 13% in Zero-shot accuracy for the 3B model). This confirms that our conclusions are robust to model scaling and implies that SFTMEM+CTX is a more effective foundation for scaling up RL-based reasoning.</text>
<section_header_level_1><loc_88><loc_176><loc_286><loc_182>D DETAILS ON SUFFICIENCY OF THE FINDING</section_header_level_1>
<text><loc_88><loc_192><loc_412><loc_260>In Section 5.1, we focus on whether our proposed recipe 'SFTMEM+CTX → RLCOMP' consistently yields superior generalization, compared to direct training on the target task 'SFTCOMP → RLCOMP' which is the common practice in real-world complex tasks. Here, we compare 'SFTMEM+CTX → RLCOMP' with other possible baselines: 1) 'SFTMEM+CTX': SFT with the entire MEM and CTX data, which is the same as in Table 1b; 2) ' SFTMEM+CTX → RLMEM+CTX ': SFT with 80% of MEM and CTX data and then RL with the rest 20% of MEM and CTX data. The portion is based on our empirical results of splitting COMP data; 3) ' SFTMEM+CTX → RLMEM+CTX → RLCOMP ': furthur RL with COMP based on 'SFTMEM+CTX → RLMEM+CTX'; 4) ' SFTMEM +CTX +COMP ': SFT with the mix of MEM, CTX and COMP data. Specifically, for RLCOMP, we adopt the 12.8k random samples used in Section 5.2. We show the performance on COMP test set in Table 7.</text>
<text><loc_88><loc_265><loc_412><loc_326>It shows that SFTMEM+CTX → RLMEM+CTX → RLCOMP has very similar performance compared with our proposed SFTMEM+CTX → RLCOMP , especially for Zero-shot setting. However, this does not contradict our conclusion, as the capture of sufficient atomic skills and RL training are essential for Zero-shot generalization and sufficient atomic skills are still the prerequisites. In addition, comparing SFTMEM+CTX with SFTMEM+CTX → RLMEM+CTX , we find that RL with atomic skills may not be beneficial to composite tasks. Note that for MEM and COMP data, every test sample in COMP would be either Composition or Zero-shot level (these two baselines have never seen any COMP data during training. Moreover, Table 7 further enhances the sufficiency of our findings that our recipe SFTMEM+CTX → RLCOMP is the key to generalization.</text>
<text><loc_88><loc_331><loc_412><loc_399>It is interesting that SFTMEM +CTX +COMP also manages to generalize to Zero-shot scenarios to some extent and achieves comparable results with our SFTMEM+CTX → RLCOMP . This shows that SFT can also generalize with careful mix of training data. However, this does not challenge our SFT Memorization Paradox that SFT directly with composite data fails to generalize. Furthermore, the setting itself is not as realistic-it is not trivial to obtain ample composite (COMP) data encompassing both MEM and CTX skills. Also, it is difficult to mix COMP data to SFT training with all atomic skills. However, our SFTMEM+CTX → RLCOMP assumes a common post-training practice with an initial SFT and a following RL stage. While the base model grasp the atomic skills ( i.e., today's available LLMs), we focus on the training strategies and data mix when we have some composite data. And we demonstrate the condition of RL generalization.</text>
<section_header_level_1><loc_88><loc_412><loc_365><loc_419>E DETAILS ON OTHER FACTORS THAT AFFECT GENERALIZATION</section_header_level_1>
<text><loc_88><loc_429><loc_412><loc_462>There are many factors affecting models'performance or generalizability. We showcase the sample efficiency and pass@k performance as some features to see why the model with atomic skills are essentials for RL generalization in composite tasks. Here, we investigate from the perspective of training dynamics of SFTMEM+CTX, the impact of training loss of the base model for RL, the embedding distributions of the SFT and RL model and the uncertainty of the model.</text>
<page_footer><loc_246><loc_476><loc_254><loc_481>19</page_footer>
<page_break>
<page_header><loc_88><loc_18><loc_145><loc_24>Work in Progress</page_header>
<picture><loc_91><loc_51><loc_409><loc_153><caption><loc_88><loc_163><loc_412><loc_182>Figure 9: Training dynamics of SFT with parametric and contextual reasoning data over training steps. Ability gains are calculated over MEM (Red Line), CTX (Blue Line) and COMP (Green Line), respectively. As the training progresses, complementary reasoning ability emerges to some extent.</caption></picture>
<picture><loc_89><loc_194><loc_410><loc_252><caption><loc_90><loc_259><loc_410><loc_265>Figure 10: The impact of training steps ( i.e., training loss) of the SFT model on RL generalization.</caption></picture>
<section_header_level_1><loc_88><loc_282><loc_257><loc_288>E.1 THE TRAINING DYNAMICS OF SFTMEM+CTX</section_header_level_1>
<text><loc_88><loc_296><loc_412><loc_323>Settings To check when and how the ability of complementary reasoning ability (COMP) emerges through the training of parametric and contextual reasoning i.e., SFTMEM+CTX, we showcase the performance of MEM, CTX and COMP during the SFT training over MEM and CTX. We study Qwen-1.5B-Base and Qwen-0.5B-Base.</text>
<text><loc_88><loc_334><loc_412><loc_381>The ability of complementary reasoning emerges to some extent with the progress of both parametric and contextual reasoning. Figure 9 shows the training dynamic over training steps. First, it shows that as the SFT training with both MEM and CTX data progresses, the ability of COMP somehow emerges with MEM and CTX to some extent. Second, the conclusion is consistent over different model sizes, with larger models generalizes better to COMP with MEM and CTX data. Third, it further demonstrates that contextual reasoning is the easiest (learned from relatively early stage), while the parametric and complementary reasoning is relatively difficult.</text>
<section_header_level_1><loc_88><loc_394><loc_316><loc_400>E.2 THE IMPACT OF TRAINING LOSS FOR RL GENERALIZATION</section_header_level_1>
<text><loc_88><loc_408><loc_412><loc_462>Weinvestigate a critical question regarding the interplay between Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL): Does the degree of SFT convergence dictate the model's potential for RL-based generalization? Specifically, we aim to determine if a 'grokking-like' phenomenon exists-where continuous optimization of SFT loss, even after apparent metric saturation, further unlocks the model's reasoning capabilities during the RL stage. Also, we study at which checkpoint ( i.e., training loss) may the model emerge the ability of generalization. We take four intermediate checkpoints of SFTMEM+CTX to further conduct RLCOMP and evaluate the performance on three levels of generalization.</text>
<page_footer><loc_246><loc_476><loc_254><loc_481>20</page_footer>
<page_break>
<page_header><loc_88><loc_18><loc_145><loc_24>Work in Progress</page_header>
<text><loc_88><loc_54><loc_412><loc_66>Figure 10 illustrates the performance trajectories across SFT checkpoints (1k to 4k steps), revealing three distinct phases of capability emergence:</text>
<unordered_list><list_item><loc_88><loc_71><loc_412><loc_98>Insufficient Representation. At the early stage ( i.e., 1k steps, Training Loss ≈ 0 . 44 ), the model has not yet internalized the necessary reasoning patterns. Consequently, it fails to generalize effectively during the RL stage, resulting in suboptimal performance across all metrics (e.g., Zero-shot accuracy is significantly lower compared to later stages).</list_item>
<list_item><loc_88><loc_103><loc_412><loc_136>Emergence of Capabilities. As the SFT loss decreases to approximately 0 . 16 ( i.e., 2k steps), we observe a sharp 'phase transition' in downstream RL performance. While the base SFT model's direct performance (dashed lines) shows only moderate improvements, its latent potential for RL adaptation increases dramatically. This suggests that the critical structures required for reasoning generalization are established during this interval.</list_item>
<list_item><loc_88><loc_141><loc_412><loc_181>Saturation and Robustness. Performance peaks around 3k steps (Loss ≈ 0 . 05 ). Interestingly, further extending training to 4k steps-where the model nearly memorizes the training data (Loss ≈ 0 . 0004 )-does not yield further significant gains, nor does it lead to performance degradation. This indicates that while 'grokking' (delayed generalization) effectively occurs between 1k and 3k steps, the benefit saturates once the loss drops below a certain threshold ( < 0 . 05 ). The model becomes robust, maintaining its high plasticity for RL even when deeply fitted to the SFT distribution.</list_item>
</unordered_list>
<text><loc_88><loc_186><loc_412><loc_206>In conclusion, minimizing SFT loss is crucial up to a point . The generalization capability for RL does not scale infinitely with lower loss but requires a sufficient 'incubation' period (up to 3k steps in our setting) to fully emerge.</text>
<section_header_level_1><loc_88><loc_219><loc_284><loc_225>E.3 ANALYSIS OF LATENT SPACE DYNAMICS VIA PCA</section_header_level_1>
<text><loc_88><loc_233><loc_412><loc_267>Setting: Layer-wise PCA Projection To investigate the internal mechanism behind the superior performance of SFTMEM+CTX → RLCOMP compared to SFTCOMP → RLCOMP, we conduct a Principal Component Analysis (PCA) on the hidden states of the models. Our goal is to visualize how different training strategies affect the representation of Parametric, Contextual, and Complementary Reasoning.</text>
<text><loc_88><loc_272><loc_412><loc_336>Formally, for a given model pair (Anchor Model M anc and Target Model M tgt ) and a specific layer l , we extract the hidden states corresponding to the last token of the input queries. Let H anc l ∈ R N × D and H tgt l ∈ R N × D denote the hidden state matrices for N queries at layer l , where D is the hidden dimension. To capture the relative shift induced by training, we fit the PCA transformation on M anc 's states H anc l , which defines a 2D coordinate system based on the principal variations of the reference model. We then project M tgt 's states H tgt l into this fixed coordinate system. The shift vector for layer l is calculated as the difference between the centroids of the projected target states and the anchor states. This process is repeated for all layers. Figure 11 shows the layer-wise shifts by scatter points. The large markers represent the global centroid shift ( z ∗ ) for each Reasoning type.</text>
<text><loc_88><loc_348><loc_412><loc_409>Disentanglement of atomic skills in SFTMEM+CTX → RLCOMP As shown in the top-left panel of Figure 11, SFTMEM+CTX exhibits a significant 'disentanglement' of atomic reasoning types. The centroid for MEM data (Blue Circle) and CTX data (Orange Triangle) move in distinct directions and magnitudes within the principal component space. This suggests that the SFTMEM+CTX stage effectively separates the internal representations required for parametric recall versus contextual reasoning. Furthermore, in the subsequent RL stage (top-right panel), this separation is maintained and refined. Notably, the COMP data (Green Square) aligns closely with the established distributions of MEM and CTX data. This indicates that the model can effectively generalize the logic learned from MEM and CTX data to the composite COMP tasks.</text>
<text><loc_88><loc_422><loc_412><loc_462>Entanglement of skills in SFTCOMP → RLCOMP. In contrast, the bottom row reveals a phenomenon of 'representation entanglement'. Please note that the scale of the axis for the bottom row is lower than that for the top row. For the model trained only on COMP data, the embeddings for MEM, CTX, and COMP queries remain tightly clustered together, after both the SFT stage (bottomleft) and the RL stage (bottom-right). The centroids for all three data types are located close to the origin with overlapping distributions. This lack of separation implies that SFTCOMP fails to distin-</text>
<page_footer><loc_246><loc_476><loc_254><loc_481>21</page_footer>
<page_break>
<page_header><loc_88><loc_18><loc_145><loc_24>Work in Progress</page_header>
<picture><loc_88><loc_51><loc_412><loc_259><caption><loc_88><loc_267><loc_412><loc_286>Figure 11: PCA Analysis of SFTMEM+CTX → RLCOMP and SFTCOMP → RLCOMP. The scatter points represent the layer-wise shifts. The large markers represent the global centroid shift ( z ∗ ) for each reasoning type.</caption></picture>
<text><loc_88><loc_310><loc_412><loc_323>guish between the underlying mechanisms of parametric and contextual reasoning, instead learning a coupled representation.</text>
<text><loc_88><loc_328><loc_412><loc_361>Wehypothesize that the superior generalization capability of SFTMEM+CTX → RL(Comp) stems from this structural disentanglement. By explicitly separating the latent representations of parametric and contextual capabilities during the SFT stage, the model establishes a robust basis that facilitates better adaptation during the RL phase. Conversely, the coupled representations in SFTCOMP limit the model's ability to distinctly apply these capabilities, leading to suboptimal performance.</text>
<section_header_level_1><loc_88><loc_381><loc_188><loc_386>E.4 MODEL UNCERTAINTY</section_header_level_1>
<text><loc_88><loc_398><loc_412><loc_417>We investigate the evolution of model uncertainty (quantified by the average prediction entropy × 100 ) to understand the underlying dynamics of RL generalization. Table 8 presents the entropy metrics across I.I.D., Compositional, and Zero-shot subsets.</text>
<text><loc_88><loc_422><loc_412><loc_462>Uncertainty is correlated with SFT convergence, not necessarily RL potential. We first address whether high uncertainty is a prerequisite for effective RL exploration. Comparing the SFTMEM+CTX checkpoints, the uncertainty drops significantly from 3k steps ( 12 . 92 ) to 4k steps ( 7 . 13 ) as the loss minimizes. Despite this lower starting entropy, we previously observed that the 4k-step model sustains high performance after RL. This indicates that a 'calibrated' and confident SFT model (lower entropy) does not hinder subsequent RL generalization.</text>
<page_footer><loc_246><loc_476><loc_254><loc_481>22</page_footer>
<page_break>
<page_header><loc_88><loc_18><loc_145><loc_24>Work in Progress</page_header>
<otsl><loc_134><loc_85><loc_365><loc_213><ched>Setting<ched>Overall<ched>I.I.D.<ched>Comp.<ched>Zero-shot<nl><srow>Effect of Training Steps (SFT on MEM + CTX )<lcel><lcel><lcel><lcel><nl><rhed>SFT MEM+CTX 3k step<fcel>12.92<fcel>13.41<fcel>12.05<fcel>13.12<nl><rhed>SFT MEM+CTX 4k step<fcel>7.13<fcel>6.33<fcel>7.45<fcel>8.46<nl><srow>10% COMP data for SFT COMP , 90% COMP data for RL COMP<lcel><lcel><lcel><lcel><nl><rhed>SFT COMP<fcel>8.93<fcel>6.56<fcel>9.89<fcel>12.96<nl><rhed>SFT COMP → RL COMP<fcel>8.99<fcel>8.49<fcel>8.97<fcel>10.17<nl><rhed>SFT MEM+CTX → RL COMP<fcel>2.58<fcel>1.63<fcel>2.54<fcel>4.84<nl><srow>30% COMP data for SFT COMP , 70% COMP data for RL COMP<lcel><lcel><lcel><lcel><nl><rhed>SFT COMP<fcel>7.18<fcel>5.24<fcel>7.98<fcel>10.46<nl><rhed>SFT COMP → RL COMP<fcel>6.11<fcel>4.85<fcel>6.36<fcel>8.65<nl><rhed>SFT COMP → RL COMP<fcel>2.90<fcel>1.96<fcel>2.84<fcel>5.17<nl><srow>90% COMP data for SFT COMP , 10% COMP data for RL COMP<lcel><lcel><lcel><lcel><nl><rhed>SFT COMP<fcel>4.14<fcel>2.38<fcel>4.05<fcel>8.36<nl><rhed>SFT COMP → RL COMP<fcel>4.11<fcel>3.27<fcel>3.97<fcel>6.25<nl><rhed>SFT COMP → RL COMP<fcel>4.10<fcel>3.23<fcel>3.88<fcel>6.46<nl><caption><loc_88><loc_52><loc_412><loc_79>Table 8: Analysis of model uncertainty (measured by average entropy × 100 ) across different training settings. Overall denotes average uncertainty on the test set, while the breakdown columns show uncertainty on I.I.D., Compositional (Comp.), and Zero-shot settings. Lower values indicate higher model confidence.</caption></otsl>
<text><loc_88><loc_227><loc_412><loc_247>Task Difficulty Indicator. Consistently across all settings, the uncertainty is highest in the Zeroshot subset (e.g., 12 . 96 for 10% SFT(G3)). This aligns with intuition, as the model exhibits lower confidence on unseen distributions.</text>
<text><loc_88><loc_252><loc_412><loc_306>SFTMEM+CTX Facilitates Efficient RL Adaptation. A key insight emerges when comparing the post-RL behaviors. In the 30% data setting, 30% SFTCOMP ( 7 . 18 ) and SFTMEM+CTX ( 7 . 13 ) start at remarkably similar uncertainty levels. However, after applying identical RL training, SFTCOMP → RLCOMP shows minimal entropy reduction ( 7 . 18 → 6 . 11 ), suggesting the model struggles to find a more optimal, confident policy. In contrast, our recipe SFTMEM+CTX → RLCOMP achieves a drastic reduction in uncertainty ( 7 . 13 → 2 . 90 ). This demonstrates that the model with sufficient atomic skills does not merely provide 'randomness' for exploration; rather, it structures the latent space (see Figure 11) in a way that allows RL to efficiently converge to a high-confidence, correct solution.</text>
<section_header_level_1><loc_88><loc_318><loc_175><loc_324>F TERMINOLOGIES</section_header_level_1>
<page_footer><loc_246><loc_476><loc_254><loc_481>23</page_footer>
<page_break>
<page_header><loc_88><loc_18><loc_145><loc_24>Work in Progress</page_header>
<otsl><loc_88><loc_186><loc_435><loc_339><ched>Symbol/Abbreviation<ched>Description<nl><fcel>MEM CTX<fcel>Parametric reasoning type. Contextual reasoning type.<nl><fcel>COMP<fcel>Complementary reasoning type.<nl><fcel>RL<fcel>Reinforcement Learning, one of the core training strategies discussed in this work.<nl><fcel>SFT<fcel>Supervised Fine-Tuning, one of the core training strategies discussed in this work.<nl><fcel>LoRA<fcel>Low-Rank Adaptation, one of the training strategies discussed in this work.<nl><fcel>SFT MEM+CTX<fcel>The model obtained after performing SFT on the combined training set of MEM and CTX.<nl><fcel>SFT COMP<fcel>The model obtained after performing SFT on the training set of COMP.<nl><fcel>SFT 10% COMP<fcel>The model obtained after performing SFT on 10% random samples of COMP training set.<nl><fcel>SFT MEM+CTX → RL COMP<fcel>The model obtained after performing RL on the training set of COMP from SFT MEM+CTX .<nl><fcel>SFT COMP → RL COMP<fcel>The model obtained after performing RL on the training set of COMP from SFT COMP .<nl><fcel>I.I.D<fcel>Independent and Identically Distributed generalization type in the test set.<nl><caption><loc_171><loc_174><loc_329><loc_180>Table 9: Glossary of Symbols and Abbreviations</caption></otsl>
<page_footer><loc_246><loc_476><loc_254><loc_481>24</page_footer>
</doctag>