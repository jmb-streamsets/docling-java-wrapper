<doctag><page_header><loc_15><loc_138><loc_30><loc_362>arXiv:2511.15706v2  [cs.CV]  20 Nov 2025</page_header>
<section_header_level_1><loc_105><loc_68><loc_395><loc_76>RoMa v2: Harder Better Faster Denser Feature Matching</section_header_level_1>
<text><loc_57><loc_92><loc_442><loc_108>Johan Edstedt 1 David Nordstr¨ om 2 Yushan Zhang 1 Georg B¨ okman 3 Jonathan Astermark 4 Viktor Larsson 4 Anders Heyden 4 Fredrik Kahl 2 M˚ arten Wadenb¨ ack 1 Michael Felsberg 1</text>
<text><loc_151><loc_110><loc_349><loc_117>1 Link¨ oping University, 2 Chalmers University of Technology</text>
<text><loc_118><loc_119><loc_380><loc_125>3 University of Amsterdam, 4 Centre for Mathematical Sciences, Lund University</text>
<section_header_level_1><loc_126><loc_145><loc_162><loc_152>Abstract</section_header_level_1>
<text><loc_48><loc_162><loc_241><loc_326>Dense feature matching aims to estimate all correspondences between two images of a 3D scene and has recently been established as the gold-standard due to its high accuracy and robustness. However, existing dense matchers still fail or perform poorly for many hard real-world scenarios, and high-precision models are often slow, limiting their applicability. In this paper, we attack these weaknesses on a wide front through a series of systematic improvements that together yield a significantly better model. In particular, we construct a novel matching architecture and loss, which, combined with a curated diverse training distribution, enables our model to solve many complex matching tasks. We further make training faster through a decoupled two-stage matching-then-refinement pipeline, and at the same time, significantly reduce refinement memory usage through a custom CUDA kernel. Finally, we leverage the recent DINOv3 foundation model along with multiple other insights to make the model more robust and unbiased. In our extensive set of experiments we show that the resulting novel matcher sets a new state-of-the-art, being significantly more accurate than its predecessors. Code is available at https://github.com/Parskatt/romav2.</text>
<section_header_level_1><loc_48><loc_345><loc_111><loc_351>1. Introduction</section_header_level_1>
<text><loc_48><loc_359><loc_241><loc_427>Dense matching is the task of matching every pixel of image I A ∈ R H A × W A × 3 , with image I B ∈ R H B × W B × 3 in terms of a warp W A ↦→ B ∈ R H A × W A × 2 and a confidence mask p A ↦→ B ∈ [0 , 1] H A × W A × 1 . In dense feature matching, the assumption is that the pixels in both images are observations of 3D points from the same scene . In this case, for a perfect matcher, the confidence p A ↦→ B is 1 for pixels corresponding to a 3D point in the scene that is observable from both views, i.e., that are co-visible, and 0 for occluded pixels.</text>
<text><loc_48><loc_430><loc_241><loc_450>Feature matching is a fundamental task in Computer Vision, as many downstream tasks, e.g., visual localization [29, 34, 42] and 3D reconstruction [19, 20, 25, 36, 38],</text>
<picture><loc_260><loc_145><loc_451><loc_255><caption><loc_259><loc_262><loc_452><loc_288>Figure 1. Radar chart of performance on benchmarks. RoMa v2 outperforms previous dense matchers on a wide range of pose estimation and dense matching tasks. Further details on these experiments can be found in Section 4.</caption></picture>
<text><loc_259><loc_299><loc_452><loc_357>rely on precise and trustworthy correspondences in order to function robustly. Traditionally, these methods relied on sparse matches established purely through descriptor similarity. In the last couple of years, these detector-descriptor methods have been gradually replaced by learning-based matchers that consider pairs of images when establishing the correspondences, allowing the networks to not only consider visual similarly but also the spatial context.</text>
<text><loc_259><loc_361><loc_452><loc_411>This development in feature matching has been driven by the introduction of several challenging benchmarks, such as MegaDepth-1500 [22, 41], ScanNet-1500 [9, 33], WxBS [27] and the recurring Image Matching Challenge at CVPR [15]. These benchmarks are currently dominated by detector-free methods, such as dense feature matchers [11] and feed-forward reconstruction models [47].</text>
<text><loc_259><loc_415><loc_452><loc_450>One noteable dense matcher is RoMa [12], which has proven robust to extreme photometric changes, including different modalities, due to using features from a frozen foundation model for the matching instead of learning features from scratch. However, RoMa still struggles in</text>
<page_footer><loc_248><loc_464><loc_252><loc_469>1</page_footer>
<page_break>
<picture><loc_47><loc_45><loc_453><loc_200><caption><loc_48><loc_206><loc_452><loc_225>Figure 2. Qualitative results. RoMa v2 excels at matching in diverse scenarios. We show a snapshot of results from different benchmarks. Below each image pair we visualize the dense warp by coloring each pixel by the RGB value from its estimated corresponding location in the opposite image. Brighter values mean lower warp confidence as output by the model.</caption></picture>
<text><loc_48><loc_232><loc_241><loc_320>many challenging scenarios. For example, the recent RUBIK benchmark [24] highlights its weakness under extreme viewpoint changes. Additionally, RoMa has a significant runtime and memory footprint, limiting its applicability for large-scale tasks or resource constrained settings. Recently, UFM [52] showed that dense matching can be made significantly faster than in RoMa. However, UFM requires finetuning of the pretrained feature extracting backbone, which leads to worse performance on datasets with extreme appearance changes such as WxBS. Furthermore, UFM performs worse than RoMa on benchmarks that require subpixel precision, such as MegaDepth-1500 [22, 41].</text>
<text><loc_48><loc_323><loc_241><loc_419>Motivated by the different trade-offs in RoMa and UFM, in this paper we address the challenge of combining their respective strengths, i.e., developing a dense feature matcher that is both robust to extreme changes in viewpoint and appearance, applicable to a wide range of real-world scenarios, all while maintaining subpixel precision and a practical runtime and memory footprint. To this end, we introduce RoMa v2, which builds on RoMa and features several improvements to increase robustness while simultaneously reducing the computational cost. RoMa v2 achieves stateof-the-art results on a wide range of benchmarks, as seen in Figure 1. Qualitative examples from the benchmarks are visualized in Figure 2.</text>
<section_header_level_1><loc_48><loc_421><loc_190><loc_427>In summary, our main contributions are:</section_header_level_1>
<ordered_list><list_item><loc_48><loc_430><loc_241><loc_450>A novel matching objective, combining warp and correlation-based losses, which enables multi-view context to be learned in the coarse matcher of RoMa, de-</list_item>
</ordered_list>
<text><loc_269><loc_232><loc_342><loc_237>scribed in Section 3.2.</text>
<ordered_list><list_item><loc_259><loc_239><loc_452><loc_252>Faster and less memory intensive refiners than in RoMa, described in Section 3.3 and ablated in Table 8.</list_item>
<list_item><loc_259><loc_254><loc_452><loc_290>A custom mixture of wide- and small-baseline datasets in the training data, that helps balance robustness to extreme viewpoints while maintaining sub-pixel performance across a wide range of difficult matching tasks, detailed in Section 3.4.</list_item>
<list_item><loc_259><loc_292><loc_452><loc_312>Prediction of pixel-wise error covariance that can be used downstream in refinement of estimated geometry, as demostrated in Section 4.6</list_item>
<list_item><loc_259><loc_315><loc_452><loc_350>We experimentally verify that these improvements significantly reduce the runtime compared to baseline RoMa, while matching or outperforming both RoMa and UFM on their respective strongpoints across a wide range of benchmarks in Section 4.</list_item>
</ordered_list>
<section_header_level_1><loc_259><loc_363><loc_327><loc_369>2. Related Work</section_header_level_1>
<text><loc_259><loc_377><loc_452><loc_450>Feature Matching: Traditionally, feature matching has been dominated by the sparse paradigm, where keypoints are first detected separately in each image, and then matched by a sparse feature matcher. While early matching was driven by similarity of local descriptors, the recent trend is instead to rely on a learned matcher that jointly considers the keypoints and descriptors in each image. Notable recent works in this area include SuperGlue [33] and LightGlue [23] which both use an attention-based approach for solving the optimal assignment. Common among the sparse</text>
<page_footer><loc_248><loc_464><loc_252><loc_469>2</page_footer>
<page_break>
<text><loc_48><loc_48><loc_241><loc_151>methods is the reliance on salient image regions, where repeatable keypoints can be detected. In contrast, LoFTR [41] takes a detector-free approach by matching through attention on learned features, resulting in a semi-dense matching where even non-salient points can be matched. In DKM[11], the matching is performed on a pyramid of feature maps, enabling pixel-dense matches. In a follow-up work, RoMa [12] further improves on this method by using a frozen foundation model to encode the coarse matching features, making it significantly more robust to extreme appearance changes. Recently, UFM [52] was introduced as a more lightweight dense matcher, where the training of wide-baseline dense matching is unified with the related task of optical flow.</text>
<text><loc_48><loc_157><loc_241><loc_276>Feedforward Reconstruction: Recovering 3D structure and camera parameters from images, or Structure-fromMotion (SfM) [14], has traditionally relied on sequential pipelines such as Bundler [38] and COLMAP [36], where point correspondences obtained through image matching play a central role. Recently, learning-based SfM methods have emerged, and many now incorporate matching within a feedforward architecture. DUSt3R [47] and MASt3R [21] directly regresses point maps from image pairs; and VGGT [46] and MapAnything [18] extend this paradigm to longer sequences. While these models can produce coarse correspondences, they struggle to yield accurate high-resolution dense matches. While our architecture is loosely inspired by these approaches we retain an explicit dense matching formulation that provides subpixel-accurate correspondences.</text>
<section_header_level_1><loc_48><loc_285><loc_90><loc_292>3. Method</section_header_level_1>
<text><loc_48><loc_299><loc_241><loc_350>In this section, we outline our proposed method. We begin by discussing our two-stage matching-then-refinement architecture in Section 3.1 and proceed to discuss its two parts in Sections 3.2 and 3.3, respectively. In Section 3.4, we describe the training data used and in Section 3.5 we explain the method used to make RoMa v2 more robust to changes in image resolution.</text>
<section_header_level_1><loc_48><loc_357><loc_112><loc_363>3.1. Architecture</section_header_level_1>
<text><loc_48><loc_369><loc_241><loc_420>Wetake inspiration from previous works [12, 52] and divide the dense matching task into a matching step and a refinement step. Intuitively, these two tasks entail first finding an approximate or coarse match for each pixel, and, conditioned on this, refining the matching to sub-pixel accuracy. An overview of the architecture is shown in Figure 3, and the respective components in Figure 4 and Figure 5.</text>
<text><loc_48><loc_422><loc_241><loc_450>While some previous works, e.g. [11, 12], decouple gradients between matchers and refiners but still train both jointly, we instead opt for a two-stage training paradigm inspired by UFM [52]. This enables rapid experimentation.</text>
<text><loc_259><loc_48><loc_452><loc_61>We next go into detail on the matcher in Section 3.2, followed by the refinement in Section 3.3.</text>
<section_header_level_1><loc_259><loc_68><loc_308><loc_74>3.2. Matcher</section_header_level_1>
<text><loc_259><loc_80><loc_452><loc_145>An overview of the coarse matcher is shown in Figure 4. We upgrade the DINOv2 [28] encoder used in RoMa to the newer DINOv3 [37]. Inspired by Edstedt et al. [12], we compare the encoders (frozen) by training a single linear layer on the features followed by a kernel nearest neighbor matcher. As is shown in Table 1, we find that DINOv3 is more robust than its predecessor despite its slightly larger patch size (16 vs. 14). As in RoMa, but unlike UFM, we freeze the encoder weights, improving robustness.</text>
<text><loc_259><loc_148><loc_452><loc_214>While RoMa is robust and generalizes well, one of its main weaknesses is its lacking multi-view context in the matcher, which relies solely on Gaussian Process (GP) [31] regression combined with a single-view Transformer decoder to classify warp bins. A naive approach would be to add a Multi-view Transformer to RoMa before the GP, however, we found that in practice the gradients through the GPwere not sufficiently informative to yield improvements, and caused stability issues during training.</text>
<text><loc_259><loc_216><loc_452><loc_297>To remedy this, we replace the Gaussian Process with a simple single headed Attention mechanism. Additionally, we add an auxiliary target, L NLL, to minimize the negative log-likelihood of the best matching patch in image B for each patch in image A. We first compute the similarity between all patches from image A and image B to form a similarity matrix S ∈ R M × N , where M and N are the number of patches in image A and B respectively. The loss L NLL is computed by first applying Softmax over the second dimension of S and then selecting the most similar pairs for all patches in A, i.e.</text>
<formula><loc_294><loc_305><loc_452><loc_324></formula>
<text><loc_259><loc_330><loc_452><loc_450>where n ∗ is the index of the patch closest to the GT warp for patch m . This approach can be seen as a dense directional version of, e.g. LoFTR [41]. We still however use a regression head, which is trained to minimize the robust regression loss between its predicted warp and the ground truth warp. The full coarse matching pipeline independently tokenizes the input frames using DINOv3 ViTL, and then applies a ViT-B Multi-view Transformer. Following Wang et al. [46], we alternate between frame-wise and global Attention. In contrast to Wang et al. [46], we do not use RoPE across frames. The final token embeddings, Softmax( S ) x B , where x B are position embeddings as in RoMa, and DINOv3 features, are jointly processed by a DPT head to predict the warp and confidence. Further details on the matcher architecture are given in the supplementary material.</text>
<page_footer><loc_248><loc_464><loc_252><loc_469>3</page_footer>
<page_break>
<picture><loc_76><loc_44><loc_423><loc_124><caption><loc_48><loc_129><loc_452><loc_186>Figure 3. Overview of RoMa v2. We estimate bidirectional dense image warps W = { W A ↦→ B ∈ R H × W × 2 , W B ↦→ A ∈ R H × W × 2 } and warp confidences p = { p A ↦→ B ∈ R H × W × 1 , p B ↦→ A ∈ R H × W × 1 } between two input images using a two-stage pipeline consisting of a matching and refinement stage. Different from recent SotA dense matchers, we additionally predict a precision matrix Σ -1 = { ( Σ -1 ) A ↦→ B ∈ R H × W × 2 × 2 , ( Σ -1 ) B ↦→ A ∈ R H × W × 2 × 2 } . The coarse matcher is a Multi-view Transformer, that takes in frozen DINOv3 [37] foundation model features from image I A ∈ R H × W × 3 and I B ∈ R H × W × 3 . Its internals are further illustrated in Figure 4, and explained in detail in Section 3.2. The refiners are fine-grained UNet-like CNN models that, conditioned on the previous warp and confidence, produce displacements and delta confidences. Besides this, they additionally predict a full 2 × 2 precision matrix per-pixel, which is visualized as ∣ ∣ Σ -1 ∣ ∣ -1 / 4 . The refiners are further illustrated in Figure 5 and explained in more detail in Section 3.3.</caption></picture>
<picture><loc_46><loc_197><loc_240><loc_254><caption><loc_48><loc_255><loc_241><loc_295>Figure 4. Coarse matcher. We use a frozen DINOv3 feature extractor in the coarse matching stage. DINOv3 features from both input images are input to a Multi-view Transformer utilizing alternating Attention. Dense Prediction Transformer (DPT) [30] heads output coarse warps W between the images and confidences p for 4x downsampled resolution.</caption></picture>
<otsl><loc_82><loc_324><loc_204><loc_352><fcel>Method<fcel>EPE ↓<fcel>Robustness% ↑<nl><fcel>DINOv2<fcel>27.1<fcel>77.0<nl><fcel>DINOv3<fcel>19.0<fcel>86.4<nl><caption><loc_48><loc_304><loc_241><loc_322>Table 1. Robustness of frozen features. We compare coarse features for matching through a linear probe on MegaDepth. Robustness is the share of matches with an error below 32 pixels.</caption></otsl>
<text><loc_48><loc_365><loc_241><loc_409>Matching Loss: We use the same overlap loss L overlap , and weighting factor, as in RoMa. However, we replace the classification-by-regression term from the matching loss for the robust regression term L warp used in the refinement loss, which is also used by UFM. Finally, we add our proposed L NLL and obtain:</text>
<formula><loc_67><loc_415><loc_241><loc_431></formula>
<text><loc_48><loc_437><loc_241><loc_451>In contrast to UFM, our matching objective incorporates the auxiliary target L NLL.We compare these architectures in</text>
<otsl><loc_272><loc_218><loc_439><loc_246><ched>Method ↓<ched>PCK @ →<ched>1 px ↑<ched>3px<ched>5px ↑<nl><rhed>UFM<ecel><fcel>11.2<fcel>48.3<fcel>67.4<nl><rhed>RoMa v2<ecel><fcel>30.5<fcel>76.7<fcel>86.7<nl><caption><loc_259><loc_199><loc_452><loc_211>Table 2. Comparing RoMa v2 and UFM matching architectures on Hypersim [32] . Measured in PCK (higher is better).</caption></otsl>
<picture><loc_258><loc_257><loc_453><loc_338><caption><loc_259><loc_344><loc_452><loc_363>Figure 5. Refiner internals. The coarse matcher predicts at a resolution 4x smaller than the original image size. The refiners output at the original resolution.</caption></picture>
<text><loc_259><loc_378><loc_452><loc_406>Table 2 by training on a subset of the data using the training setup outlined below and evaluating on holdout scenes from Hypersim [32]. We find that the RoMa v2 matcher significantly improves robustness.</text>
<text><loc_259><loc_422><loc_452><loc_450>Coarse Matching Training: We start by training the coarse matcher for 300k steps of batch size 128, resulting in approximately 38M pairs seen throughout training. We use a learning rate of 4 · 10 -4 .</text>
<page_footer><loc_248><loc_464><loc_252><loc_469>4</page_footer>
<page_break>
<section_header_level_1><loc_48><loc_47><loc_95><loc_53>3.3. Refiners</section_header_level_1>
<text><loc_48><loc_59><loc_241><loc_94>After the matcher has finished training, we freeze it and run it in inference mode, producing a coarse warp for the refiners to refine. We train the refiners for 300k steps of batch size 64, resulting in a total of approximately 19M pairs. Like the matcher, we use a learning rate of 4 · 10 -4 .</text>
<text><loc_48><loc_99><loc_241><loc_196>Architecture: As the matcher, in contrast to RoMa where the matcher predicts at stride 14, predicts at stride 4, there is only a need to refine at strides equal or smaller. We thus construct three refiners at strides { 4 , 2 , 1 } respectively. These follow a similar architecture as in RoMa [12] with some efficiency improvements. First, we find that the local correlation implementation used in RoMa uses a large amount of memory, especially at high resolution. To remedy this we write a custom CUDA kernel as a PyTorch extension, which significantly reduces the memory consumption (cf. Table 8). We further change all channel dimensions to be powers of two, which further boosts performance. Further details about the refiners is given in the appendix.</text>
<text><loc_48><loc_201><loc_241><loc_252>Predictive Covariance: It is often useful, besides a notion of predicted overlap, to have access to a numerical estimate of the expected error. While predictive uncertainty has been previously studied [16, 35, 44, 51, 55], State-ofthe-Art methods such as RoMa [12] or UFM [52] do not provide any such estimate. To remedy this, we predict a pixel-wise Gaussian uncertainty of the 2D residuals,</text>
<formula><loc_80><loc_259><loc_241><loc_267></formula>
<text><loc_48><loc_274><loc_241><loc_326>through a 2 × 2 precision matrix P θ ∈ R H × W × 2 × 2 where Σ -1 θ ( h, w ) ≻ 0 , i.e., Σ -1 θ ( h, w ) is positive definite. We ensure this by constraining the network to predict the three elements z 11 , z 21 , z 22 and mapping these to Cholesky factors as l 11 = Softplus( z 11 ) + 10 -6 , l 21 = z 21 , l 22 = Softplus( z 22 )+10 -6 , where Softplus( · ) = ln(1+exp( · )) . The lower triangular matrix is composed from the factors as</text>
<formula><loc_117><loc_333><loc_241><loc_348></formula>
<text><loc_48><loc_356><loc_241><loc_376>and then the covariance matrix is formed from this as Σ -1 = LL ⊤ . To learn z 11 , z 21 , z 22 we directly train the model to minimize the negative log-likelihood</text>
<formula><loc_54><loc_385><loc_241><loc_408></formula>
<text><loc_48><loc_414><loc_241><loc_435>To ensure stability, we only train the model to predict this covariance for covisible regions where ∥ r ∥ < 8 pixels. We additionally detach the residuals r θ before the loss.</text>
<text><loc_48><loc_437><loc_241><loc_450>We predict the precision in an hierarchical fashion from stride 4 up to stride 1, and use that fact that information</text>
<picture><loc_263><loc_47><loc_448><loc_108><caption><loc_259><loc_115><loc_452><loc_141>Figure 6. Subpixel bias of refinement. We observe that models exhibit subpixel fluctuations in their predictions throughout training, leading to bias. We propose a simple remedy through storing an exponential moving average (EMA).</caption></picture>
<text><loc_259><loc_158><loc_452><loc_171>is additive in the precision parameterization to predict our final precision matrix as</text>
<formula><loc_324><loc_179><loc_452><loc_194></formula>
<text><loc_259><loc_203><loc_452><loc_223>We find empirically that our covariance improves performance in downstream tasks (cf. Table 10), and that it qualitatively behaves as one would expect in Figure 9.</text>
<text><loc_259><loc_230><loc_452><loc_318>EMA to remedy bias: During training, we empirically observed that predictions tend to have a small, but noticeable, sub-pixel bias (typically around ± 0 . 1 pixels in resolution 640 × 640 ). At first this seemed like a data issue, but through plotting this bias over the course of training we found that it appears almost random, see Figure 6a. As the bias is seemingly uncorrelated over the course of training, a simple way to fix it is to simply use an Exponential Moving Average (EMA) 1 . We found a decay factor of α = 0 . 999 to work well empirically. After applying this remedy, we find that the bias in both orientations is substantially diminished, see Figure 6b.</text>
<text><loc_259><loc_325><loc_452><loc_346>Refinement Loss: We train the refiners using a combination of three losses. Following RoMa we use a generalized Charbonnier loss [2] which for each refiner reads</text>
<formula><loc_301><loc_355><loc_452><loc_371></formula>
<text><loc_259><loc_377><loc_452><loc_437>where we follow RoMa and set α = 0 . 5 , c = 10 -3 and i ∈ { 4 , 2 , 1 } is the stride. For estimating the overlap we follow UFM and RoMa and use a pixel-wise binary cross-entropy loss as L overlap, with the ground truth overlap p GT ∈ { 0 , 1 } being derived from either consistent depth (for MVS style datasets) or from warp cycle consistency (for flow datasets). Further details on computing ground truth warps and overlaps are given in the suppl. material.</text>
<footnote><loc_268><loc_445><loc_444><loc_450>1 See Izmailov et al. [17] for discussion regarding different variants.</footnote>
<page_footer><loc_248><loc_464><loc_252><loc_469>5</page_footer>
<page_break>
<picture><loc_48><loc_45><loc_240><loc_155><caption><loc_48><loc_162><loc_241><loc_188>Figure 7. Finegrained objects in warp. Left: RoMa warp for small baseline pair. Note the missing warp for the guitar in the bottom right. Right: RoMa v2 warp. RoMa v2 is significantly better at capturing small objects with dynamic motion.</caption></picture>
<text><loc_48><loc_199><loc_136><loc_204>The total refinement loss is</text>
<formula><loc_68><loc_211><loc_241><loc_248></formula>
<section_header_level_1><loc_48><loc_255><loc_82><loc_261>3.4. Data</section_header_level_1>
<text><loc_48><loc_266><loc_241><loc_294>Wetrain RoMa v2 on a mix of wide and small baseline twoview datasets, a summary of which is presented in Table 3. Our mix is inspired by UFM [52], and significantly more diverse than RoMa [12], which is only trained on MegaDepth.</text>
<text><loc_48><loc_297><loc_241><loc_400>In particular, the inclusion of the Aerial datasets AerialMD [45] and BlendedMVS [49], enable our proposed model to be significantly more robust to large rotations and air-to-ground viewpoint changes. The inclusion of small-baseline datasets, like FlyingThings3D [26], makes RoMa v2 significantly better at predicting finegrained details. We qualitatively compare RoMa v2 to RoMa on fine-grained prediction on the FlyingThings3D dataset in Figure 7. We also find that our data mixture enables us to predict textureless surface significantly better than RoMa, particularly in Autonomous Driving (AD) scenarios, despite only training on the very small-scale dataset VKITTI2 [6, 13]. We demonstrate this for a randomly selected pair from the NuScenes dataset [7] in Figure 8.</text>
<section_header_level_1><loc_48><loc_407><loc_104><loc_414>3.5. Resolution</section_header_level_1>
<text><loc_48><loc_419><loc_241><loc_432>We find that some elementary key changes make matching and refinement robust to the choice of resolution.</text>
<text><loc_48><loc_437><loc_241><loc_450>Coarse Matching: Following DINOv3 [37] we use RoPE [40] on a normalized grid, rather than a pixel grid.</text>
<picture><loc_260><loc_44><loc_452><loc_155><caption><loc_259><loc_162><loc_452><loc_188>Figure 8. Better texture-poor geometry prediction. Left: RoMa warp for large-baseline pair with texture-poor road surfaces, with warp missing for almost the entire road. Right: RoMa v2 warp. RoMa v2 predicts accurately for covisible road.</caption></picture>
<otsl><loc_259><loc_233><loc_453><loc_329><ched>Datasets<ched>Type / GT Source<ched>Weight<ched>№ Scenes<nl><rhed>MegaDepth [22]<fcel>Outdoor / MVS<fcel>1<fcel>169<nl><rhed>AerialMD [45]<fcel>Aerial / MVS<fcel>1<fcel>124<nl><rhed>BlendedMVS [49]<fcel>Aerial / Mesh<fcel>1<fcel>493<nl><rhed>Hypersim [32]<fcel>Indoor / Graphics<fcel>1<fcel>393<nl><rhed>TartanAir v2 [48]<fcel>Outdoor / Graphics<fcel>1<fcel>46<nl><rhed>Map-Free [1]<fcel>Object-centric / MVS<fcel>1<fcel>397<nl><rhed>ScanNet++ v2 [50]<fcel>Indoor / Mesh<fcel>1<fcel>856<nl><rhed>UnrealStereo4k [43]<fcel>Outdoor / Graphics<fcel>0.01<fcel>8<nl><rhed>Virtual KITTI 2 [6, 13]<fcel>Outdoor / Graphics<fcel>0.01<fcel>5<nl><rhed>FlyingThings3D [26]<fcel>Outdoor / Graphics<fcel>0.5<fcel>2239<nl><rhed>Total<ecel><ecel><fcel>5069<nl><caption><loc_259><loc_193><loc_452><loc_225>Table 3. Dataset mixture for RoMa v2. The top part contains wide-baseline datasets, while the bottom part contains smallbaseline datasets. The weight is proportional to the probability of sampling an image pair from the respective dataset. Further details about the datasets is provided in the supplementary.</caption></otsl>
<text><loc_259><loc_342><loc_452><loc_355>This ensures that distances are always in distribution, even when changing resolution significantly.</text>
<text><loc_259><loc_358><loc_452><loc_416>Secondly, we find that the absolute position encodings used for the match embeddings need to be of low enough frequency, to ensure that their interpolation is unproblematic [53]. Compared to RoMa, which initializes the scale to ω = 8 , and let it be trained, we set it fixed to ω = 1 . It is possible that the high frequency of the position embeddings is the cause of the issue which requires UFM to be run at a fixed resolution of 420 × 560 during inference.</text>
<text><loc_259><loc_422><loc_452><loc_450>Refinement: Ensuring resolution robustness for the refiners is non-trivial, as convolution is tied to the pixel grid. We find that the approach used in RoMa, whereby the input displacement is rescaled relative to a canonical resolution</text>
<page_footer><loc_248><loc_464><loc_252><loc_469>6</page_footer>
<page_break>
<otsl><loc_60><loc_72><loc_227><loc_152><ched>Method ↓<ched>AUC @ →<ched>5 ◦ ↑<ched>10 ◦ ↑<ched>20 ◦ ↑<nl><rhed>Reloc3r [10] CVPR'25<lcel><fcel>49.6<fcel>67.9<fcel>81.2<nl><rhed>MASt3R [21] ECCV'24<lcel><fcel>42.4<fcel>61.5<fcel>76.9<nl><rhed>VGGT † [46] CVPR'25<lcel><fcel>33.5<fcel>52.9<fcel>70.0<nl><rhed>LightGlue [23] ICCV'23<lcel><fcel>51.0<fcel>68.1<fcel>80.7<nl><rhed>LoFTR [41] CVPR'21<lcel><fcel>52.8<fcel>69.2<fcel>81.2<nl><rhed>DKM[11] CVPR'23<lcel><fcel>60.4<fcel>74.9<fcel>85.1<nl><rhed>RoMa [12] CVPR'24<lcel><fcel>62.6<fcel>76.7<fcel>86.3<nl><rhed>UFM † [52] NeurIPS'25<lcel><fcel>41.5<fcel>57.9<fcel>72.4<nl><rhed>RoMa v2<lcel><fcel>62.8<fcel>77.0<fcel>86.6<nl><caption><loc_48><loc_46><loc_241><loc_65>Table 4. SotA comparison on MegaDepth-1500 [22, 41] . The top part contains feed-forward 3D reconstruction models, while the bottom part contains feature matchers.</caption></otsl>
<footnote><loc_65><loc_153><loc_135><loc_158>† Our reproduced numbers.</footnote>
<text><loc_48><loc_166><loc_114><loc_172>generalizes the best.</text>
<text><loc_48><loc_177><loc_241><loc_206>Training: We train the coarse matcher on a mix of resolutions and aspect ratios, specifically: { 512 × 512 , 592 × 448 , 624 × 416 , 688 × 384 , 448 × 592 , 416 × 624 , 384 × 688 } . The refiners are trained exclusively with size 640 × 640 .</text>
<section_header_level_1><loc_48><loc_215><loc_111><loc_222>4. Experiments</section_header_level_1>
<text><loc_48><loc_229><loc_241><loc_249>The qualitative improvements of RoMa v2 as shown in Figures 2, 7 and 8 are confirmed by the quantitative results from extensive experiments, listed in this section.</text>
<section_header_level_1><loc_48><loc_257><loc_158><loc_263>4.1. Relative Pose Estimation</section_header_level_1>
<text><loc_48><loc_269><loc_241><loc_334>Wecompare RoMa v2 to state-of-the-art matchers and feedforward 3D reconstruction methods on relative pose estimation. For sampling correspondences we follow RoMa and compute bidirectional warps from which we sample correspondences in a thresholded distribution where we set ˆ p A ↦→ B = max( 1 p A ↦→ B > 0 . 05 , p A ↦→ B ) . We use a coarse resolution of 800 × 800 and a fine resolution of 1024 × 1024 . Similarly, we also sample a balanced subset of matches using their kernel density estimate approach.</text>
<text><loc_48><loc_337><loc_241><loc_388>We report results on MegaDepth-1500 [22, 41] in Table 4 and ScanNet-1500 [9, 33] in Table 5. RoMa v2 consistently outperforms all prior matchers on both benchmarks. On MegaDepth, which demands accurate sub-pixel correspondences, RoMa v2 also surpasses all 3D reconstruction methods. On ScanNet, RoMa v2 achieves performance on par with VGGT and MASt3R.</text>
<section_header_level_1><loc_48><loc_395><loc_125><loc_402>4.2. Dense Matching</section_header_level_1>
<text><loc_48><loc_407><loc_241><loc_450>We evaluate dense matching performance in Table 6 and Table 7 on a wide array of datasets and compare to stateof-the-art dense matchers RoMa and UFM. For RoMa and RoMa v2, we directly feed the 640 × 640 images into the model, while for UFM we first resize the image to their suggested inference resolution 560 × 420 and then bilinearly</text>
<otsl><loc_271><loc_72><loc_438><loc_159><ched>Method ↓<ched>AUC @ →<ched>5 ◦ ↑ 10 ◦<ched>20 ◦ ↑<nl><rhed>Reloc3r [10] CVPR'25<fcel>34.8<fcel>58.4<fcel>75.6<nl><rhed>MASt3R † [21] ECCV'24<fcel>33.6<fcel>56.8<fcel>74.1<nl><rhed>VGGT [46] CVPR'25<fcel>33.9<fcel>55.2<fcel>73.4<nl><rhed>LightGlue [23] ICCV'23<fcel>17.8<fcel>34.0<fcel>52.0<nl><rhed>LoFTR [41] CVPR'21<fcel>22.1<fcel>40.8<fcel>57.6<nl><rhed>DKM[11] CVPR'23<fcel>29.4<fcel>50.7<fcel>68.3<nl><rhed>RoMa[12] CVPR'24<fcel>31.8<fcel>53.4<fcel>70.9<nl><rhed>UFM [52] NeurIPS'25<fcel>31.6<fcel>54.1<fcel>-<nl><rhed>UFM † [52] NeurIPS'25<fcel>31.3<fcel>54.1<fcel>72.0<nl><rhed>RoMa v2<fcel>33.6<fcel>56.2<fcel>73.8<nl><caption><loc_259><loc_46><loc_452><loc_65>Table 5. SotA comparison on ScanNet-1500 [9, 33] . The top part contains feed-forward 3D reconstruction models, while the bottom part contains feature matchers.</caption></otsl>
<footnote><loc_277><loc_160><loc_347><loc_165>† Our reproduced numbers.</footnote>
<text><loc_259><loc_172><loc_452><loc_185>upsample the predictions back to 640 × 640 , as their precision degrades significantly for higher resolution.</text>
<text><loc_259><loc_187><loc_452><loc_223>RoMa v2 consistently outperforms across all 6 datasets. Notably, we simultaneously beat UFM on its own benchmark, TA-WB, and RoMa on MegaDepth, on which it is exclusively trained, while having 84% lower EPE compared to RoMa on the challenging AerialMegaDepth benchmark.</text>
<section_header_level_1><loc_259><loc_229><loc_361><loc_235>4.3. Runtime Comparisons</section_header_level_1>
<text><loc_259><loc_240><loc_452><loc_276>In Table 8 we compare the runtime of RoMa v2 with RoMa and UFM. As can be seen from the table, we improve the throughput significantly compared to RoMa, running 1 . 7 × faster. Compared to UFM our model is slightly slower, however with a much smaller memory memory footprint.</text>
<section_header_level_1><loc_259><loc_290><loc_401><loc_296>4.4. Multi-Modal Matching on WxBS</section_header_level_1>
<text><loc_259><loc_301><loc_452><loc_382>We evaluate the robustness of RoMa v2 on the extremely challenging WxBS benchmark [27]. This benchmark consists of handlabeled correspondences between images taken with extreme changes in either viewpoint, illumination, modality, or all three, which measures the generalizability of the matcher to out-of-distribution downstream tasks. Results are presented in Table 9. We observe that the performance of RoMa v2 is significantly higher than UFM, but slightly lower than RoMa. Investigating the cause of this we found that RoMa v2 and UFM both struggle with the IR-to-RGB multi-modal subset of WxBS.</text>
<section_header_level_1><loc_259><loc_388><loc_421><loc_394>4.5. Astronaut to Satellite Image Matching</section_header_level_1>
<text><loc_259><loc_399><loc_452><loc_450>We introduce a new benchmark, SatAst, for matching images taken by astronauts from the international space station to satellite images. Prior work on this modality has focused on the retrieval task of searching a database of satellite images for the image content of a given astronaut image [35, 39]. We take 39 corresponding image pairs from EarthMatch [3] to create SatAst and hand-annotate 10 accurate</text>
<page_footer><loc_248><loc_464><loc_252><loc_469>7</page_footer>
<page_break>
<otsl><loc_69><loc_56><loc_431><loc_98><ched>Method<ched>TA-WB<lcel><lcel><ched>MegaDepth<lcel><lcel><lcel><lcel><ched>ScanNet++ v2<lcel><lcel><lcel><nl><ucel><ched>EPE ↓<ched>1px ↑<ched>3px ↑<ched>5px ↑<ched>EPE ↓<ched>1px ↑<ched>3px ↑<ched>5px ↑<ched>EPE ↓<ched>1px ↑<ched>3px ↑<ched>5px ↑<nl><rhed>RoMa<fcel>60.61<fcel>35.1<fcel>52.6<fcel>56.2<fcel>2.34<fcel>74.8<fcel>93.7<fcel>96.4<fcel>27.52<fcel>20.2<fcel>42.8<fcel>53.6<nl><rhed>UFM<fcel>15.85<fcel>31.3<fcel>65.5<fcel>75.1<fcel>3.15<fcel>55.3<fcel>88.0<fcel>93.7<fcel>6.93<fcel>31.4<fcel>67.7<fcel>80.0<nl><rhed>RoMa v2<fcel>13.82<fcel>67.7<fcel>81.8<fcel>85.8<fcel>1.47<fcel>79.6<fcel>94.7<fcel>96.7<fcel>4.00<fcel>45.5<fcel>77.3<fcel>86.6<nl><caption><loc_131><loc_46><loc_369><loc_51>Table 6. Dense matching performance. Images are resized to 640 × 640 pixels.</caption></otsl>
<otsl><loc_68><loc_115><loc_430><loc_157><ched>Method<ched>FlyingThings3D<lcel><lcel><ched>AerialMegaDepth<lcel><lcel><lcel><lcel><ched>MapFree<lcel><lcel><lcel><nl><ucel><ched>EPE ↓<ched>1px ↑<ched>3px ↑<ched>5px ↑<ched>EPE ↓<ched>1px ↑<ched>3px ↑<ched>5px ↑<ched>EPE ↓<ched>1px ↑<ched>3px ↑<ched>5px ↑<nl><rhed>RoMa<fcel>5.68<fcel>78.0<fcel>86.6<fcel>89.2<fcel>25.05<fcel>39.0<fcel>65.0<fcel>73.9<fcel>8.55<fcel>45.8<fcel>72.3<fcel>80.9<nl><rhed>UFM<fcel>1.33<fcel>83.4<fcel>93.9<fcel>96.1<fcel>17.44<fcel>29.3<fcel>61.6<fcel>73.8<fcel>3.59<fcel>31.6<fcel>66.7<fcel>81.7<nl><rhed>RoMa v2<fcel>0.93<fcel>89.4<fcel>95.2<fcel>96.8<fcel>4.12<fcel>55.9<fcel>81.1<fcel>87.9<fcel>2.03<fcel>55.4<fcel>84.9<fcel>92.7<nl><caption><loc_118><loc_106><loc_382><loc_111>Table 7. Further dense matching performance. Images are resized to 640 × 640 pixels.</caption></otsl>
<caption><loc_48><loc_167><loc_241><loc_194>Table 8. Runtime and memory. Benchmarking on 640 × 640 † images with a batch size of 8 on an H200. RoMa v2 is 1 . 7 × faster than RoMa with similar memory footprint. K indicates the custom CUDA kernel for the local correlation operation.</caption>
<otsl><loc_48><loc_200><loc_238><loc_243><ched>Method<ched>Throughput (pairs/s) ↑<ched>Mem. (GB) ↓<nl><fcel>UFM<fcel>43.0<fcel>16.2<nl><fcel>RoMa<fcel>18.5<fcel>4.7<nl><fcel>RoMa v2 (w/o K )<fcel>30.3<fcel>5.6<nl><fcel>RoMa v2 (w/ K )<fcel>30.9<fcel>4.8<nl></otsl>
<text><loc_54><loc_243><loc_216><loc_248>† We use 644 × 644 for RoMa and UFM due to patch size 14.</text>
<otsl><loc_52><loc_274><loc_234><loc_309><ched>Method<ched>WxBs (mAA @ 10px)<ched>SatAst (AUC @ 10px)<nl><fcel>RoMa<fcel>60.8<fcel>23.5<nl><fcel>UFM<fcel>42.3<fcel>1.8<nl><fcel>RoMa v2<fcel>55.4<fcel>37.0<nl><caption><loc_48><loc_255><loc_241><loc_267>Table 9. SotA comparison on the WxBS [27] and SatAst benchmarks. mAA and AUC at 10px respectively (higher is better).</caption></otsl>
<text><loc_48><loc_315><loc_241><loc_359>correspondences for each pair. Further, we include 90 degree rotated copies of the satellite images, yielding a total of 156 image pairs. Given estimated correspondences from a model, we use RANSAC to obtain a homography and compute AUC@10px of the reprojection error of the annotated ground-truth correspondences using this homography.</text>
<text><loc_48><loc_361><loc_241><loc_404>SatAst is difficult, the most challenging aspects being i) satellite images are OOD for most matchers (including RoMa v2), ii) large scale changes and iii) large in-plane rotations. We compare RoMa v2 against previous dense methods and present results in Table 9. More information about the benchmark is found in the supplementary material.</text>
<section_header_level_1><loc_48><loc_411><loc_143><loc_417>4.6. Covariance Estimate</section_header_level_1>
<text><loc_48><loc_422><loc_241><loc_450>While most robust pose estimation pipelines assume identically distributed residuals, our predictive covariance can be used to reweight residuals. To highlight the usefulness, we perform an experiment leveraging the covariance-weighted</text>
<otsl><loc_259><loc_193><loc_453><loc_229><fcel>Method ↓ AUC @ →<fcel>1 ◦ ↑<fcel>3 ◦ ↑<fcel>5 ◦ ↑<nl><rhed>RoMa v2 (w/o Σ - 1 )<fcel>54.9<fcel>79.5<fcel>85.9<nl><rhed>RoMa v2 (w/ Σ - 1 Refine)<fcel>75.8<fcel>89.0<fcel>92.6<nl><rhed>RoMa v2 (w/ Σ - 1 RANSAC + Refine)<fcel>76.4<fcel>89.3<fcel>92.8<nl><caption><loc_259><loc_168><loc_452><loc_187>Table 10. Impact of predictive covariance on Hypersim [32] . Measured in AUC (higher is better). We use the predicted covariance to weight the residuals in the model refinement.</caption></otsl>
<picture><loc_258><loc_237><loc_453><loc_288><caption><loc_259><loc_295><loc_452><loc_321>Figure 9. Qualitative example of covariances. We plot the predicted covariance for randomly sampled keypoints. In the right image, we have applied a linear kernel to simulate motion blur, yielding larger covariances, especially in the blur direction.</caption></picture>
<text><loc_259><loc_327><loc_452><loc_400>residuals. First, only as post-processing, refining the output of a classic point-based RANSAC. Second, we compare with using it to reweight the residuals used for scoring inside RANSAC. For the experiment we consider image pairs sampled from the HyperSim [32] dataset. In Table 10 we show that we gain significant improvements on 3D pose error metrics. In particular, we improve by ≈ 20 points on AUC@1. Further experimental details can be found in the supplementary material. In Figure 9 we show a qualitative example of the predicted covariances.</text>
<section_header_level_1><loc_259><loc_409><loc_393><loc_416>5. Limitations and Future Work</section_header_level_1>
<text><loc_259><loc_422><loc_452><loc_450>Compared to RoMa, our model is slightly less robust to extreme changes in modality, such as in WxBS. However, we are significantly more robust to these changes than UFM, as indicated by Table 9. Exploring the trade-offs between</text>
<page_footer><loc_248><loc_464><loc_252><loc_469>8</page_footer>
<page_break>
<text><loc_48><loc_48><loc_241><loc_61>generalization and maximizing performance is an interesting direction for future work.</text>
<section_header_level_1><loc_48><loc_70><loc_104><loc_77>6. Conclusion</section_header_level_1>
<text><loc_48><loc_84><loc_241><loc_112>We have introduced RoMa v2, a new dense feature matcher capable of matching harder pairs, with better (i.e., more precise) predictions, and with faster runtime than its predecessor, leading to denser matches.</text>
<section_header_level_1><loc_48><loc_121><loc_129><loc_128>Acknowledgements</section_header_level_1>
<text><loc_48><loc_134><loc_241><loc_209>This work was supported by the Wallenberg Artificial Intelligence, Autonomous Systems and Software Program (WASP), funded by the Knut and Alice Wallenberg Foundation, and by the strategic research environment ELLIIT, funded by the Swedish government. The computational resources were provided by the National Academic Infrastructure for Supercomputing in Sweden (NAISS) at C3SE, partially funded by the Swedish Research Council through grant agreement no. 2022-06725, and by the Berzelius resource, provided by the Knut and Alice Wallenberg Foundation at the National Supercomputer Centre.</text>
<section_header_level_1><loc_48><loc_225><loc_93><loc_232>References</section_header_level_1>
<ordered_list><list_item><loc_51><loc_238><loc_241><loc_270>Eduardo Arnold, Jamie Wynn, Sara Vicente, Guillermo Garcia-Hernando, Aron Monszpart, Victor Prisacariu, Daniyar Turmukhambetov, and Eric Brachmann. Map-free visual relocalization: Metric pose relative to a single image. In European Conf. Computer Vision (ECCV) , 2022. 6</list_item>
<list_item><loc_51><loc_273><loc_241><loc_292>Jonathan T Barron. A general and adaptive robust loss function. In IEEE Conf. Computer Vision and Pattern Recognition (CVPR) , 2019. 5</list_item>
<list_item><loc_51><loc_295><loc_241><loc_328>Gabriele Berton, Gabriele Goletto, Gabriele Trivigno, Alex Stoken, Barbara Caputo, and Carlo Masone. Earthmatch: Iterative coregistration for fine-grained localization of astronaut photography. In IEEE Conf. Computer Vision and Pattern Recognition (CVPR) , 2024. 7, 2</list_item>
<list_item><loc_51><loc_331><loc_241><loc_356>Gabriele Berton, Alex Stoken, Barbara Caputo, and Carlo Masone. Earthloc: Astronaut photography localization by indexing earth from space. In IEEE Conf. Computer Vision and Pattern Recognition (CVPR) , 2024.</list_item>
<list_item><loc_51><loc_359><loc_241><loc_385>Georg B¨ okman, Johan Edstedt, Michael Felsberg, and Fredrik Kahl. Steerers: A framework for rotation equivariant keypoint descriptors. In IEEE Conf. Computer Vision and Pattern Recognition (CVPR) , 2024. 7</list_item>
<list_item><loc_51><loc_388><loc_241><loc_400>Yohann Cabon, Naila Murray, and Martin Humenberger. Virtual kitti 2. arXiv preprint arXiv:2001.10773 , 2020. 6</list_item>
<list_item><loc_51><loc_403><loc_241><loc_435>Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: A multimodal dataset for autonomous driving. In IEEE Conf. Computer Vision and Pattern Recognition (CVPR) , 2020. 6</list_item>
<list_item><loc_51><loc_438><loc_241><loc_450>Wojciech Chojnacki, Michael J. Brooks, Anton Van Den Hengel, and Darren Gawley. On the fitting of surfaces</list_item>
<list_item><loc_275><loc_48><loc_452><loc_60>to data with covariances. IEEE Trans. Pattern Analysis and Machine Intelligence (T-PAMI) , 22(11):1294-1303, 2000. 3</list_item>
<list_item><loc_263><loc_63><loc_452><loc_95>Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In IEEE Conf. Computer Vision and Pattern Recognition (CVPR) , 2017. 1, 7</list_item>
<list_item><loc_259><loc_98><loc_452><loc_130>Siyan Dong, Shuzhe Wang, Shaohui Liu, Lulu Cai, Qingnan Fan, Juho Kannala, and Yanchao Yang. Reloc3r: Large-scale training of relative camera pose regression for generalizable, fast, and accurate visual localization. In IEEE Conf. Computer Vision and Pattern Recognition (CVPR) , 2025. 7</list_item>
<list_item><loc_259><loc_133><loc_452><loc_159>Johan Edstedt, Ioannis Athanasiadis, M˚ arten Wadenb¨ ack, and Michael Felsberg. DKM: Dense kernelized feature matching for geometry estimation. In IEEE Conf. Computer Vision and Pattern Recognition (CVPR) , 2023. 1, 3, 7</list_item>
<list_item><loc_259><loc_162><loc_452><loc_187>Johan Edstedt, Qiyu Sun, Georg B¨ okman, M˚ arten Wadenb¨ ack, and Michael Felsberg. RoMa: Robust dense feature matching. In IEEE Conf. Computer Vision and Pattern Recognition (CVPR) , 2024. 1, 3, 5, 6, 7</list_item>
<list_item><loc_259><loc_190><loc_452><loc_216>Adrien Gaidon, Qiao Wang, Yohann Cabon, and Eleonora Vig. Virtual worlds as proxy for multi-object tracking analysis. In IEEE Conf. Computer Vision and Pattern Recognition (CVPR) , 2016. 6</list_item>
<list_item><loc_259><loc_218><loc_452><loc_237>Richard Hartley and Andrew Zisserman. Multiple view geometry in computer vision . Cambridge university press, 2003. 3</list_item>
<list_item><loc_259><loc_240><loc_452><loc_258>Addison Howard, Eduard Trulls, Kwang Moo Yi, Dmitry Mishkin, Sohier Dane, and Yuhe Jin. Image matching challenge 2022, 2022. 1</list_item>
<list_item><loc_259><loc_261><loc_452><loc_287>Eddy Ilg, Ozgun Cicek, Silvio Galesso, Aaron Klein, Osama Makansi, Frank Hutter, and Thomas Brox. Uncertainty estimates and multi-hypotheses networks for optical flow. In European Conf. Computer Vision (ECCV) , 2018. 5</list_item>
<list_item><loc_259><loc_290><loc_452><loc_315>Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon Wilson. Averaging weights leads to wider optima and better generalization. arXiv preprint arXiv:1803.05407 , 2018. 5</list_item>
<list_item><loc_259><loc_318><loc_452><loc_371>Nikhil Keetha, Norman M¨ uller, Johannes Sch¨ onberger, Lorenzo Porzi, Yuchen Zhang, Tobias Fischer, Arno Knapitsch, Duncan Zauss, Ethan Weber, Nelson Antunes, Jonathon Luiten, Manuel Lopez-Antequera, Samuel Rota Bul` o, Christian Richardt, Deva Ramanan, Sebastian Scherer, and Peter Kontschieder. MapAnything: Universal feedforward metric 3D reconstruction, 2025. arXiv preprint arXiv:2509.13414. 3</list_item>
<list_item><loc_259><loc_374><loc_452><loc_393>Dmytro Kotovenko, Olga Grebenkova, and Bj¨ orn Ommer. Edgs: Eliminating densification for efficient convergence of 3dgs. arXiv preprint arXiv:2504.13204 , 2025. 1</list_item>
<list_item><loc_259><loc_395><loc_452><loc_414>JongMin Lee and Sungjoo Yoo. Dense-sfm: Structure from motion with dense consistent matching. In IEEE Conf. Computer Vision and Pattern Recognition (CVPR) , 2025. 1</list_item>
<list_item><loc_259><loc_417><loc_452><loc_436>Vincent Leroy, Yohann Cabon, and J´ erˆ ome Revaud. Grounding image matching in 3d with mast3r. In European Conf. Computer Vision (ECCV) , 2024. 3, 7</list_item>
<list_item><loc_259><loc_438><loc_452><loc_450>Zhengqi Li and Noah Snavely. Megadepth: Learning singleview depth prediction from internet photos. In IEEE Conf.</list_item>
</ordered_list>
<page_footer><loc_248><loc_464><loc_252><loc_469>9</page_footer>
<page_break>
<text><loc_64><loc_48><loc_241><loc_60>Computer Vision and Pattern Recognition (CVPR) , 2018. 1, 2, 6, 7</text>
<ordered_list><list_item><loc_48><loc_63><loc_241><loc_82>Philipp Lindenberger, Paul-Edouard Sarlin, and Marc Pollefeys. LightGlue: Local Feature Matching at Light Speed. In IEEE Int'l Conf. Computer Vision (ICCV) , 2023. 2, 7</list_item>
<list_item><loc_48><loc_85><loc_241><loc_111>Thibaut Loiseau and Guillaume Bourmaud. Rubik: A structured benchmark for image matching across geometric challenges. In IEEE Conf. Computer Vision and Pattern Recognition (CVPR) , 2025. 2</list_item>
<list_item><loc_48><loc_114><loc_241><loc_126>Mapillary. Opensfm. https://github.com/ mapillary/OpenSfM , 2014. 1</list_item>
<list_item><loc_48><loc_130><loc_241><loc_162>Nikolaus Mayer, Eddy Ilg, Philip Hausser, Philipp Fischer, Daniel Cremers, Alexey Dosovitskiy, and Thomas Brox. A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation. In IEEE Conf. Computer Vision and Pattern Recognition (CVPR) , 2016. 6</list_item>
<list_item><loc_48><loc_166><loc_241><loc_184>Dmytro Mishkin, Jiri Matas, Michal Perdoch, and Karel Lenc. WxBS: Wide baseline stereo generalizations. In British Machine Vision Conference (BMVC) , 2015. 1, 7, 8</list_item>
<list_item><loc_48><loc_188><loc_241><loc_255>Maxime Oquab, Timoth´ ee Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, ShangWen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. DINOv2: Learning robust visual features without supervision. In Int'l Conf. Learning Representations (ICLR) , 2023. 3</list_item>
<list_item><loc_48><loc_258><loc_241><loc_284>Vojtech Panek, Qunjie Zhou, Yaqing Ding, S´ ergio Agostinho, Zuzana Kukelova, Torsten Sattler, and Laura Leal-Taix´ e. A guide to structureless visual localization. arXiv preprint arXiv:2504.17636 , 2025. 1</list_item>
<list_item><loc_48><loc_287><loc_241><loc_306>Ren´ e Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. In IEEE Conf. Computer Vision and Pattern Recognition (CVPR) , 2021. 4, 1</list_item>
<list_item><loc_48><loc_310><loc_241><loc_328>Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning) . The MIT Press, 2005. 3</list_item>
<list_item><loc_48><loc_332><loc_241><loc_371>Mike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit Kumar, Miguel Angel Bautista, Nathan Paczan, Russ Webb, and Joshua M. Susskind. Hypersim: A photorealistic synthetic dataset for holistic indoor scene understanding. In IEEE Int'l Conf. Computer Vision (ICCV) , 2021. 4, 6, 8, 2</list_item>
<list_item><loc_48><loc_375><loc_241><loc_407>Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. Superglue: Learning feature matching with graph neural networks. In IEEE Conf. Computer Vision and Pattern Recognition (CVPR) , 2020. 1, 2, 7</list_item>
<list_item><loc_48><loc_411><loc_241><loc_450>Torsten Sattler, Will Maddern, Carl Toft, Akihiko Torii, Lars Hammarstrand, Erik Stenborg, Daniel Safari, Masatoshi Okutomi, Marc Pollefeys, Josef Sivic, Fredrik Kahl, and Tomas Pajdla. Benchmarking 6dof outdoor visual localization in changing conditions. In IEEE Conf. Computer Vision and Pattern Recognition (CVPR) , 2018. 1</list_item>
<list_item><loc_259><loc_48><loc_452><loc_74>Philipp Schr¨ oppel, Jan Bechtold, Artemij Amiranashvili, and Thomas Brox. A benchmark and a baseline for robust multiview depth estimation. In Int'l Conf. 3D Vision (3DV) , 2022. 5</list_item>
<list_item><loc_259><loc_77><loc_452><loc_95>Johannes Lutz Sch¨ onberger and Jan-Michael Frahm. Structure-from-Motion Revisited. In IEEE Conf. Computer Vision and Pattern Recognition (CVPR) , 2016. 1, 3</list_item>
<list_item><loc_259><loc_98><loc_452><loc_158>Oriane Sim´ eoni, Huy V. Vo, Maximilian Seitzer, Federico Baldassarre, Maxime Oquab, Cijo Jose, Vasil Khalidov, Marc Szafraniec, Seungeun Yi, Micha¨ el Ramamonjisoa, Francisco Massa, Daniel Haziza, Luca Wehrstedt, Jianyuan Wang, Timoth´ ee Darcet, Th´ eo Moutakanni, Leonel Sentana, Claire Roberts, Andrea Vedaldi, Jamie Tolan, John Brandt, Camille Couprie, Julien Mairal, Herv´ e J´ egou, Patrick Labatut, and Piotr Bojanowski. Dinov3. arXiv preprint arXiv:2508.10104 , 2025. 3, 4, 6</list_item>
<list_item><loc_259><loc_161><loc_452><loc_180>Noah Snavely, Steven M Seitz, and Richard Szeliski. Modeling the world from internet photo collections. Int'l J. Computer Vision (IJCV) , 80(2), 2008. 1, 3</list_item>
<list_item><loc_259><loc_183><loc_452><loc_208>Alex Stoken and Kenton Fisher. Find my astronaut photo: Automated localization and georectification of astronaut photography. In IEEE Conf. Computer Vision and Pattern Recognition (CVPR) Workshops , 2023. 7, 2</list_item>
<list_item><loc_259><loc_211><loc_452><loc_230>Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding, 2023. 6</list_item>
<list_item><loc_259><loc_233><loc_452><loc_258>Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. In IEEE Conf. Computer Vision and Pattern Recognition (CVPR) , 2021. 1, 2, 3, 7</list_item>
<list_item><loc_259><loc_261><loc_452><loc_294>Hajime Taira, Masatoshi Okutomi, Torsten Sattler, Mircea Cimpoi, Marc Pollefeys, Josef Sivic, Tomas Pajdla, and Akihiko Torii. Inloc: Indoor visual localization with dense matching and view synthesis. In IEEE Conf. Computer Vision and Pattern Recognition (CVPR) , 2018. 1</list_item>
<list_item><loc_259><loc_297><loc_452><loc_322>Fabio Tosi, Yiyi Liao, Carolin Schmitt, and Andreas Geiger. Smd-nets: Stereo mixture density networks. In Conference on Computer Vision and Pattern Recognition (CVPR) , 2021. 6</list_item>
<list_item><loc_259><loc_325><loc_452><loc_351>Prune Truong, Martin Danelljan, Radu Timofte, and Luc Van Gool. PDC-Net+: Enhanced Probabilistic Dense Correspondence Network. IEEE Trans. Pattern Analysis and Machine Intelligence (T-PAMI) , 2023. 5</list_item>
<list_item><loc_259><loc_354><loc_452><loc_386>Khiem Vuong, Anurag Ghosh, Deva Ramanan, Srinivasa Narasimhan, and Shubham Tulsiani. Aerialmegadepth: Learning aerial-ground reconstruction and view synthesis. In IEEE Conf. Computer Vision and Pattern Recognition (CVPR) , 2025. 6</list_item>
<list_item><loc_259><loc_389><loc_452><loc_422>Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. Vggt: Visual geometry grounded transformer. In IEEE Conf. Computer Vision and Pattern Recognition (CVPR) , 2025. 3, 7, 5</list_item>
<list_item><loc_259><loc_424><loc_452><loc_450>Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vision made easy. In IEEE Conf. Computer Vision and Pattern Recognition (CVPR) , 2024. 1, 3</list_item>
</ordered_list>
<page_footer><loc_246><loc_464><loc_254><loc_469>10</page_footer>
<page_break>
<ordered_list><list_item><loc_48><loc_48><loc_241><loc_81>Wenshan Wang, Delong Zhu, Xiangwei Wang, Yaoyu Hu, Yuheng Qiu, Chen Wang, Yafei Hu, Ashish Kapoor, and Sebastian Scherer. Tartanair: A dataset to push the limits of visual slam. In IEEE/RSJ Int'l Conf. Intelligent Robots and Systems (IROS) , 2020. 6</list_item>
<list_item><loc_48><loc_83><loc_241><loc_116>Yao Yao, Zixin Luo, Shiwei Li, Jingyang Zhang, Yufan Ren, Lei Zhou, Tian Fang, and Long Quan. Blendedmvs: A large-scale dataset for generalized multi-view stereo networks. In IEEE Conf. Computer Vision and Pattern Recognition (CVPR) , 2020. 6</list_item>
<list_item><loc_48><loc_119><loc_241><loc_144>Chandan Yeshwanth, Yueh-Cheng Liu, Matthias Nießner, and Angela Dai. Scannet++: A high-fidelity dataset of 3d indoor scenes. In IEEE Int'l Conf. Computer Vision (ICCV) , 2023. 6</list_item>
<list_item><loc_48><loc_147><loc_241><loc_172>Zhichao Yin, Trevor Darrell, and Fisher Yu. Hierarchical discrete distribution decomposition for match density estimation. In IEEE Conf. Computer Vision and Pattern Recognition (CVPR) , 2019. 5</list_item>
<list_item><loc_48><loc_175><loc_241><loc_215>Yuchen Zhang, Nikhil Keetha, Chenwei Lyu, Bhuvan Jhamb, Yutian Chen, Yuheng Qiu, Jay Karhade, Shreyas Jha, Yaoyu Hu, Deva Ramanan, Sebastian Scherer, and Wenshan Wang. Ufm: A simple path towards unified dense correspondence with flow. In Advances in Neural Information Processing Systems (NeurIPS) , 2025. 2, 3, 5, 6, 7</list_item>
<list_item><loc_48><loc_217><loc_241><loc_243>Jie Zhao, Xin Chen, Yongsheng Yuan, Michael Felsberg, Dong Wang, and Huchuan Lu. Efficient motion prompt learning for robust visual tracking. In Int'l Conf. Machine learning (ICML) , 2025. 6</list_item>
<list_item><loc_48><loc_246><loc_241><loc_278>Xiaoming Zhao, Xingming Wu, Weihai Chen, Peter C. Y. Chen, Qingsong Xu, and Zhengguo Li. Aliked: A lighter keypoint and descriptor extraction network via deformable transformation. IEEE Transactions on Instrumentation & Measurement , 72, 2023. 5</list_item>
<list_item><loc_48><loc_281><loc_241><loc_299>Huizhong Zhou, Benjamin Ummenhofer, and Thomas Brox. Deeptam: Deep tracking and mapping. In European Conf. Computer Vision (ECCV) , 2018. 5</list_item>
</ordered_list>
<page_footer><loc_246><loc_464><loc_254><loc_469>11</page_footer>
<page_break>
<section_header_level_1><loc_105><loc_45><loc_395><loc_54>RoMa v2: Harder Better Faster Denser Feature Matching</section_header_level_1>
<section_header_level_1><loc_193><loc_62><loc_307><loc_69>Supplementary Material</section_header_level_1>
<text><loc_259><loc_81><loc_452><loc_139>Refiners: We use a modified version of the refiners proposed in DKM and RoMa [11, 12]. In particular, we retain only the refiners at stride [4 , 2 , 1] , due to matching at stride 4 . This has the effect of making the refinement significantly cheaper, as we also only have to extract features from the VGG19 backbone until stride 4, compared to RoMa and DKMwhich require features and refinement from stride 16 . We denote the fine features as</text>
<section_header_level_1><loc_48><loc_80><loc_147><loc_86>A. Architectural Details</section_header_level_1>
<text><loc_48><loc_93><loc_241><loc_152>Here we give further details on the exact dimensions of inputs and outputs of the different components of our model. Matcher: The matcher takes in a list of features from the DINOv3 ViT-L backbone, in our implementation specifically layers 11 and 17, each have dimension D DINO = 1024 , and which we denote as f A { 11 , 17 } , f B { 11 , 17 } . These features are concatenated into a 2048 -dimensional feature, which is linearly projected into a 768 -dimensional subspace as</text>
<formula><loc_67><loc_156><loc_241><loc_165></formula>
<formula><loc_67><loc_167><loc_241><loc_175></formula>
<text><loc_48><loc_180><loc_241><loc_231>The projected features from image A and image B are then stacked and fed into an alternating Attention Multiview Transformer of ViT-B architecture (we use a standard implementation with dim=768, depth=12, num heads=12, ffn ratio=4, and do not employ LayerScale, we however retain the 1024 output dim through a linear map to conform to ViT-L) as</text>
<formula><loc_59><loc_235><loc_241><loc_243></formula>
<text><loc_48><loc_248><loc_241><loc_276>This Transformer alternates between global Attention, processing both frames jointly without any positional encoding, and frame-wise Attention using normalized Axial RoPE (as in DINOv3).</text>
<text><loc_48><loc_278><loc_241><loc_291>The output of m θ is used to construct the similarity matrix S ∈ R M × N as</text>
<formula><loc_88><loc_296><loc_241><loc_304></formula>
<text><loc_48><loc_308><loc_241><loc_345>where τ = 1 / 10 is the temperature following RoMa, and cossim denotes cosine similarity, i.e., cossim ( x, y ) = x · y ∥ x ∥ ∥ y ∥ where · is the dot product. Using this similarity matrix, we compute so-called 'match embeddings' (following the nomenclature of RoMa) as</text>
<formula><loc_102><loc_349><loc_241><loc_357></formula>
<text><loc_48><loc_361><loc_241><loc_397>where χ B n = cos(2 πωWx B n ) ⊕ sin(2 πωWx B n ) ∈ R 1024 , x B n ∈ R 2 is the pixel-coordinate of patch n in image B , ω = 1 (as discussed in the main text), and W ∈ R 512 × 2 is a non-learnable matrix with elements drawn from N (0 , 1) . We combine features into input for a DPT [30] head as</text>
<formula><loc_56><loc_402><loc_241><loc_410></formula>
<text><loc_48><loc_415><loc_241><loc_450>where we set the finest resolution to a quarter of the original image size. We use a scratch dimension of 256 and out dimensions of [256 , 512 , 1024 , 1024] for strides [4 , 8 , 16 , 32] respectively for the DPT head. The final prediction is made at stride 4 .</text>
<formula><loc_267><loc_146><loc_452><loc_161></formula>
<text><loc_259><loc_163><loc_452><loc_199>where the dimensions come from the raw VGG19 features (extracted right before the corresponding maxpool) projected with linear layers of sizes R 192 × 256 , R 48 × 128 , R 12 × 64 . Refiners at stride i take input of the form.</text>
<formula><loc_288><loc_206><loc_452><loc_214></formula>
<formula><loc_301><loc_216><loc_452><loc_224></formula>
<text><loc_259><loc_231><loc_452><loc_313>where at each pixel x a local corr uses the previous warp to construct a k i × k i local correlation around W A ↦→ B ( x A ) , and g i are linear maps. We use [ k 4 , k 2 , k 1 ] = [7 , 3 , 0 (no corr) ] , g 4 = R 79 × 2 , g 2 = R 23 × 2 , g 1 = R 8 × 2 . The concatenation of all these features sum up to 512 , 128 , 32 respectively for strides 4 , 2 , 1 , which are intentionally powers of two, as this slightly increases inference speed. The internals are as in DKM and RoMa, that is, 8 layers each consisting of 5 × 5 depthwise convolution, followed by BatchNorm, ReLU, and 1 × 1 pointwise convolution.</text>
<section_header_level_1><loc_259><loc_323><loc_385><loc_329>B. Further Details on Datasets</section_header_level_1>
<text><loc_259><loc_336><loc_452><loc_432>MegaDepth and AerialMegaDepth: We follow the setup in RoMa [12], which is the following. For each scene, directional overlaps are first computed using the number of shared 3D tracks divided by the number of observed tracks, giving a number between 0 and 1. Up to 200000 pairs are selected from each scene by randomly sampling up to 100000 pairs with overlap > 0 . 01 , and up to 100000 pairs with overlap > 0 . 35 . Different from other datasets, sampling is not done uniformly over scenes. Rather, sampling is done over pairs, but pair sampling likelihood is downweighted by the number of pairs in the scene to the power of 0 . 75 . Note that if the power had been 1 this would be equivalent to uniform sampling.</text>
<text><loc_259><loc_437><loc_452><loc_450>MapFree We run COLMAP's MVS on all scenes with default settings, giving us per image depth maps. Like</text>
<page_footer><loc_248><loc_464><loc_252><loc_469>1</page_footer>
<page_break>
<text><loc_48><loc_48><loc_241><loc_76>MegaDepth we compute overlaps as the directional percentage of shared 3D tracks between images. For training we sample pairs with overlap > 0 . 01 uniformly over the scenes, using only seq0 per-scene.</text>
<text><loc_48><loc_80><loc_241><loc_161>ScanNet++ v2: We train on the nvs sem train split which consists of 856 indoor scenes from which we use the DSLR images. For each scene we render image aligned depth maps from the scene mesh, which is derived from a Faro Focus Premium laser scanner. We compute the overlap pairs of images as the geometric mean of their respective directional depth map overlaps in 512 × 512 resolution. We use a threshold of 0.2 for the minimum required overlap. For each scene we compute 10000 pairs that fulfill the overlap threshold, and use them for training. This gives us ≈ 8 · 10 6 total pairs.</text>
<text><loc_48><loc_165><loc_241><loc_201>TartanAir V2: We follow the setup in UFM and use their TA-WB pairs for training, where we like UFM leave out the OldScandinavia, Sewerage, Supermarket, DesertGasStation, and PolarSciFi scenes for test. For further details about the pair construction, see Zhang et al. [52].</text>
<text><loc_48><loc_204><loc_241><loc_216>BlendedMVS: We follow MapAnything and exclude the scenes:</text>
<unordered_list><list_item><loc_55><loc_218><loc_160><loc_221>-5692a4c2adafac1f14201821,</list_item>
<list_item><loc_55><loc_224><loc_160><loc_227>-5864a935712e2761469111b4,</list_item>
<list_item><loc_55><loc_230><loc_160><loc_233>-59f87d0bfa6280566fb38c9a,</list_item>
<list_item><loc_55><loc_236><loc_160><loc_239>-58a44463156b87103d3ed45e,</list_item>
<list_item><loc_55><loc_242><loc_160><loc_245>-5c2b3ed5e611832e8aed46bf,</list_item>
<list_item><loc_55><loc_247><loc_160><loc_251>-5bf03590d4392319481971dc,</list_item>
<list_item><loc_55><loc_253><loc_160><loc_257>-00000000000000000000001a,</list_item>
<list_item><loc_55><loc_259><loc_160><loc_263>-00000000000000000000000c,</list_item>
<list_item><loc_55><loc_265><loc_160><loc_269>-000000000000000000000000.</list_item>
</unordered_list>
<text><loc_48><loc_272><loc_241><loc_285>We train on all other scenes. We use pairs with directional overlap (computed from the depth maps) larger than 0 . 05 .</text>
<text><loc_48><loc_289><loc_241><loc_310>Hypersim: We train on scenes with index < 50 , and validate on scenes with index ≥ 50 . We sample pairs with a unidirectional overlap (based on depth maps) ≥ 0 . 2 .</text>
<text><loc_48><loc_314><loc_241><loc_335>FlyingThings3D: We use the official 'TRAIN' 'TEST' split and train on both the 'clean' and 'final'-pass images, converting the provided optical flows into warps.</text>
<text><loc_48><loc_339><loc_241><loc_360>UnrealStereo4K: We train on all scenes and use the left/right images with their disparities, which we convert to warps through the disparity/depth inverse relation.</text>
<text><loc_48><loc_364><loc_241><loc_408>Virtual KITTI 2: We train on all scenes, using subsequent frames with the same condition and camera, and deriving the warp from the provided optical flow. During training we randomly draw conditions (weather conditions and camera rig position), and using either left or right stereo camera.</text>
<section_header_level_1><loc_48><loc_416><loc_198><loc_423>C. Further Details on Training Data</section_header_level_1>
<text><loc_48><loc_429><loc_241><loc_450>Augmentations: Besides using different aspect ratios, we additionally employ light data augmentation. Specifically, we use horizontal flipping, grayscale with probability</text>
<text><loc_259><loc_48><loc_452><loc_83>0.1, multiplicative brightness (ratio between [1/1.5, 1.5]), and hue jitter ( [ -15 ◦ , 15 ◦ ] in the HSV parameterization). For MegaDepth and AerialMegaDepth we additionally follow RoMa and translate the image randomly in the range {-32 , ..., 32 } in both rows and columns.</text>
<text><loc_259><loc_87><loc_452><loc_115>Visualization of Training Batch: We visualize a randomly drawn batch in from the training data in Figure 10, in order to give a qualitative understanding of the type of pairs RoMa v2 is trained on.</text>
<text><loc_259><loc_118><loc_452><loc_154>Overlap/Covisibility Computation: For depth-based datasets (all datasets except FlyingThings3D, UnrealStereo4k, and Virtual KITTI 2) we use depth consistency to compute pixel-wise covisibility. We say that the depth is consistent if</text>
<formula><loc_298><loc_158><loc_452><loc_173></formula>
<text><loc_259><loc_178><loc_452><loc_200>where x A ↦→ B is the mapping of the pixel-coordinate x A into B as x A ↦→ B ∼ K B ( R A ↦→ B ( K A ) -1 x A + t ) and z A ↦→ B = ( K B ( R A ↦→ B ( K A ) -1 x A + t )) 3 is the corresponding depth.</text>
<text><loc_259><loc_202><loc_452><loc_215>For flow-based datasets, we measure the warp cycle consistency as</text>
<formula><loc_263><loc_219><loc_452><loc_227></formula>
<text><loc_259><loc_232><loc_353><loc_238>at a resolution of 640 × 640 .</text>
<section_header_level_1><loc_259><loc_246><loc_403><loc_253>D. Further Details on Benchmarks</section_header_level_1>
<text><loc_259><loc_259><loc_452><loc_347>SatAst: We create a new matching benchmark called SatAst (Satellite, Astronaut), that uses images taken by astronauts from the international space station and satellite images. We take 39 pairs of corresponding images from EarthMatch [3] (which they in turn took from AIMS [39]). The pairs in EarthMatch were obtained by retrieving ten satellite images from a large database for each given astronaut image. For our benchmark we only select image pairs that are correctly matching (confirmed by visual inspection). We also exclude images with extreme cloud occlusions as well as images where we were not able to accurately annotate correspondences that agree on a homography.</text>
<text><loc_259><loc_349><loc_452><loc_362>We annotate corresponding points in the image pairs in an iterative fashion as illustrated in Figure 11.</text>
<text><loc_259><loc_364><loc_452><loc_400>To get a sense of how good the annotations are, we estimate homographies from them and calculate the reprojection error from mapping the points from the astronaut image through the homography to the satellite image. The resulting errors are plotted in Figure 12.</text>
<section_header_level_1><loc_259><loc_408><loc_452><loc_423>E. Further Details on Predictive Covariance Experiment</section_header_level_1>
<text><loc_259><loc_430><loc_452><loc_450>We create a benchmark out of 1500 pairs from validation scenes the HyperSim [32] dataset, where pairs with < 0 . 2 overlap are discarded.</text>
<page_footer><loc_248><loc_464><loc_252><loc_469>2</page_footer>
<page_break>
<picture><loc_48><loc_44><loc_452><loc_344><caption><loc_72><loc_351><loc_428><loc_356>Figure 10. Visualization of training batch. Our data mixures is diverse and challenging, covering many types of scenes.</caption></picture>
<text><loc_48><loc_372><loc_241><loc_415>Since RoMa v2 predicts only the forward covariance (and the residuals are two-sided), we approximate the full 4 × 4 covariance matrix of the matches by a block diagonal matrix where for each drawn correspondence pair x A , x B the covariance of points in I A are approximated by sampling the backwards covariance as</text>
<formula><loc_65><loc_443><loc_241><loc_450></formula>
<text><loc_269><loc_372><loc_452><loc_377>Weoptimize the covariance weighted Sampson error [8],</text>
<formula><loc_304><loc_395><loc_452><loc_412></formula>
<text><loc_259><loc_428><loc_452><loc_450>where ∥ u ∥ 2 Σ = u T Σ u . For the robust estimation experiment, we similarly use the residual inside the MSAC scoring, inside a standard LO-RANSAC.</text>
<page_footer><loc_248><loc_464><loc_252><loc_469>3</page_footer>
<page_break>
<picture><loc_97><loc_46><loc_402><loc_407><caption><loc_48><loc_414><loc_452><loc_446>Figure 11. Annotation of correspondences for SatAst: 1) We annotate four initial approximate correspondences. 2) We warp the satellite image using the homography obtained from the previous step and annotate ten accurate correspondences. 3) Visualization of the warp obtained by estimating a homography from the ten accurate annotations. 4) The ten accurate correspondences visualized in the original images, where we score the homographies obtained from the dense matchers. Step 2) is sometimes repeated several times until a warp that is deemed good enough is obtained.</caption></picture>
<page_footer><loc_248><loc_464><loc_252><loc_469>4</page_footer>
<page_break>
<picture><loc_67><loc_45><loc_223><loc_146><caption><loc_48><loc_155><loc_241><loc_195>Figure 12. Accuracy of annotations on SatAst: Ahistogram over the reprojection errors of the 390 annotated correspondences in SatAst according to homographies estimated from the ten annnotations in each image. The image resolution of the satellite images is 3072 × 3072 , so an error of 10 pixels is around 0 . 3% of the image width.</caption></picture>
<section_header_level_1><loc_48><loc_209><loc_162><loc_216>F. Relative Pose Estimation</section_header_level_1>
<text><loc_48><loc_222><loc_241><loc_266>VGGT: For evaluating VGGT on MegaDepth-1500, we follow the evaluation outlined by Wang et al. [46]. In particular, we sample 1024 keypoints using ALIKED [54] and use as query points in the tracking head. We try different confidence and covisibility thresholds. We settle for 0 . 1 for both.</text>
<text><loc_48><loc_269><loc_241><loc_298>UFM: We follow the same sampling as for RoMa v2 and RoMa, using bidirectional warps and balanced sampling. However, as UFM only supports a fixed resolution of H = 420 × W = 560 we do not use upsampling.</text>
<section_header_level_1><loc_48><loc_306><loc_167><loc_313>G. Bias In AerialMegaDepth</section_header_level_1>
<text><loc_48><loc_319><loc_241><loc_355>We observe that RoMa v2 sometimes produces estimates overlaps in textureless sky regions, as demonstrated in Figure 13 We believe that this is due to AerialMegaDepth sometimes propagating depths from the mesh to sky pixels, as illustrated in Figure 14.</text>
<picture><loc_259><loc_59><loc_452><loc_190><caption><loc_259><loc_198><loc_452><loc_217>Figure 13. Visualization of RoMa v2 warp. Note that the model puts some confidence erroneously in sky pixels (see top-left). This may be due to bias stemming from AerialMegaDepth.</caption></picture>
<picture><loc_259><loc_252><loc_452><loc_400><caption><loc_259><loc_407><loc_452><loc_433>Figure 14. Spurious depth estimates in AerialMegaDepth. Depth from the scene leaks into the sky, causing some skypixels to be multi-view consistent. This possibly leaks into the warp estimate of RoMa v2.</caption></picture>
<page_footer><loc_248><loc_464><loc_252><loc_469>5</page_footer>
</doctag>